\section{Segmentation of Terrain Types}
\label{sec:segmentation}

Now that all fundamentals are covered, this chapter focuses on the implementation of the neural networks. Each reference architecture from section~\ref{sec:ref_archs} will be trained and evaluated with the original hyperparameters first. Afterwards, slight modifications are applied to the architectures, to look for optimization opportunities for the task at hand.

All implementations are done in Python with the popular Keras framework backed by TensorFlow~\cite{tf_whitepaper15}. It is a well established framework with extensive tooling, lots of documentation and a great open-source community. Also it is proven to be production-ready by a broad variety of companies for all kinds of tasks\footnote{Case studies can be found at \url{https://www.tensorflow.org/about/case-studies}.}. More information about tools and hardware used throughout this thesis is listed in appendix~\ref{app:tools_hardware}

\subsection{Preparing the training and test dataset}
\label{sec:prepare_train_test}

Since the goal is to have a model that is able to generalize, it is recommended to measure its performance on data that it has not seen during training. Therefore, the dataset is split in two disjoint sets of data called \emph{training set} and \emph{test set}~\cite[p.~101f]{DLbook16}. During training, only samples from the training set are presented to the model. Afterwards, the performance is measured with samples from the test set. That way, models which just memorize the training examples perform rather bad compared to models which learn features on a generalized level.

The test set should only be used to evaluate the performance, but not to decide on hyperparameters or architecture of the model. Otherwise, the model may become too closely aligned with the test set, i.~e. it is no longer possible to evaluate the generalization ability of the model with it. For such decisions another split of the training set is made, which is then called \emph{validation set}~\cite[p.~119]{DLbook16}. This set is used to estimate the generalization error during training and optimization without consulting the test set. Only after all optimizations have been done and no further changes to the model are planned, the test set is used to evaluate the final performance of the model.

There are many ways to pick the samples for the test and validation sets~\cite{val_split18}. provides an insight into commonly used techniques. For this thesis, the test set respects the class imbalance that is found within the full dataset (see section~\ref{sec:dataset_considerations}). This is to achieve fair evaluation results which are not biased by any particular class. For each class, $10\%$ of the samples containing this class were chosen and added to the test set. The remaining samples are randomly distributed among training and validation set at a ratio of $9:1$.

\subsection{Tackling class imbalance}
\label{sec:class_imbalance}

As already expressed in section~\ref{sec:dataset_considerations}, there is a large imbalance between the segmentation classes in the dataset. This can lead to undesirable biases when training a model with such a dataset. For example, if the model predicts the \texttt{forest} class for every pixel all the time, the categorical accuracy would be at $65\%$ measured over the entire dataset. While this number does not sound too bad, the actual segmentation results would be useless.

There are some techniques to tackle the issues that come with an imbalanced dataset. One approach is to add class specific weights for the loss function during training~\cite{class_imbalance19}. The weights will affect the outcome of the loss function and therefore also the gradients during backpropagation. They are assigned antiproportionally to the distribution of the classes, i.~e. overrepresented classes are given small weights and underrepresented classes are given big weights. This has the same effect as duplicating the underrepresented samples in the training set, but it is computationally more efficient.

One major challenge is to find appropriate class weights, so that the imbalances are minimized as much as possible. For the given dataset, this can be done by taking into account the respective surface area for each class. Many different sets of class weights were tested, but none of them proved to be well suited. All trained models ended up predicting only a single class for all pixels all the time. Hence the class weights were dropped and not pursued any further.

At a closer look on the dataset, most segments are huge contiguous patches of the same class. The export of the image tiles as described in section~\ref{sec:image_export} results in a high number of images which only cover one single class. This might have a major impact on the learning outcome. Since most of the samples presented to the model only consist of a single class, it is very likely that the model aligns to this characteristics. So instead of learning to differentiate between segments with precise boundaries, the model might rather be directed to roughly classify larger areas.

For this reason, a different approach was chosen to address the class imbalance issues. Since the overall goal for the model is to differentiate between classes, images containing only a single class do not provide much value for training. Thus, all image tiles where all pixels belong to the same class are dropped entirely. On the one hand, this creates incentives for the model to better recognize the differences between classes and the characteristics of class boundaries. On the other hand, this also affects class imbalance, since large partitions of forests and agricultural areas are dropped, but mostly small streets and rivers are preserved. The first few attempts with the reduced training set looked    very promising. Hence, this strategy will be used further on for the training of all the models.

\subsection{Experiments with U-Net}
\label{sec:unet_experiments}
The reference architecture that was explored first is the U-Net as introduced in section~\ref{sec:unet}. The default input size for images was set to $572\times 572$ pixels. Those are the same dimensions as the authors of U-Net used in their initial approach~\cite{unet15}. Because of the unpadded convolution layers, the dimensions of the predictions are smaller than the original input image. For the standard configuration in U-Net the output size is $388\times 388$ pixels. Depending on the number of convolution layers, these dimensions may vary.

All U-Net models were trained with a stochastic gradient descent optimizer. The initial learning rate was set to $0.1$ and reduced after every epoch, to finetune the weights towards the end of the training. A batch size of $4$ was used in combination with Nesterov momentum~\cite{nesterov83} of $0.99$ to speed up the training process and avoid oscillations. The loss was computed using a categorical crossentropy function.

Starting from the model with its original hyperparameter configuration, a few other configurations have been explored. To distinguish between the configurations, a naming convention is established. The original configuration is called \texttt{U-Net-23D}, where the number represents the exact number of convolution layers in the model. The suffix \texttt{D} indicates that a dropout layer was added before the expansive path to provide regularization during the training.

\subsubsection{Metrics}
During training, some metrics were collected to measure the predictive performance of the models (see section~\ref{sec:metrics} for details on the metrics). The plots in figure~\ref{fig:unet_train_metrics} present the epoch-wise loss, categorical accuracy and mean IoU for each U-Net configuration. The computation of the metrics was performed using the validation set. This set only contains samples that were not fed into the model for updating its weights. Thus, the metrics provide good evidence for the model's ability to generalize.

\begin{figure}
    \newcommand{\UnetMetricsImageWidth}{0.32\textwidth}
    \centering
    \input{content/tikz/unet_metrics}
    \caption{Validation Set Metrics of U-Net Configurations}
    \label{fig:unet_train_metrics}
\end{figure}

It is clearly visible that \texttt{U-Net-28D} does not perform well. For all three metrics it yields the worst results. Also, it does not show any significant improvement over time, which is why the training was stopped after five epochs. On a closer investigation of the bad performance, it was found that the model always predicts the \texttt{agriculture} class for every pixel. It might be the case that because of the additional convolution layers the architecture became to deep, so that the correlation between input and output values got lost on their way.

The configuration \texttt{U-Net-18D} shows some improvements regarding the metrics in the first few epochs of training. After the fifth epoch the graphs flatten out and show only minor improvements. The training was stopped after ten epochs in total, where it hits around $60.8\%$ categorical accuracy and $21.6\%$ mean IoU. The \texttt{U-Net-18D} configuration was the one with the least number of parameters. Fewer parameters directly affect the ability to learn and generalize a broad range of features. The assumption is that the capacity of the model is not sufficient to comprehend the full space of features involved in this challenge.

The best results with regards to the metrics achieve both \texttt{U-Net-23D} and \texttt{U-Net-23}. They both show similar rates of improvements, which level out after $25$ epochs. The configuration with dropout layers features a slightly faster learning behavior, but both arrive at around $74\%$ categorical accuracy and $42.2\%$ mean IoU in the end.

The metrics already allow to draw some conclusions about the configurations. However, it is useful to also look at the visual results before starting the discussion.

\subsubsection{Images}
While the metrics provide a quick overview over larger parts of the dataset, looking at concrete samples offers a deeper insight into the model's behavior in certain situations. Figure~\ref{fig:unet_prediction_images} depicts predictions in the test set from some configurations, together with the original image and the ground truth. The red square in the original image indicates the area that is covered in the prediction. As already raised in section~\ref{sec:dataset_considerations}, the ground truth labels are not super accurate with regards to the original image.

\begin{figure}[h]
    \newcommand{\UnetPredictionsImageWidth}{0.18\textwidth}
    \centering

    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/82607-image} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/82607-label} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-18D/82607-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23/82607-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23D/82607-prediction}

    \vspace{4mm}
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/104483-image} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/104483-label} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-18D/104483-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23/104483-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23D/104483-prediction}

    \vspace{4mm}
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/114975-image} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/114975-label} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-18D/114975-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23/114975-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23D/114975-prediction}

    \vspace{4mm}
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/138625-image} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/138625-label} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-18D/138625-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23/138625-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23D/138625-prediction}

    \vspace{4mm}
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/147352-image} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/147352-label} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-18D/147352-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23/147352-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23D/147352-prediction}

    \begin{tikzpicture}
        \node[align=center] at (-1.2, 0) {\tiny Input};
        \node[align=center] at (2.0, 0) {\tiny Ground Truth};
        \node[align=center] at (5.0, 0) {\tiny U-Net-18D};
        \node[align=center] at (8.1, 0) {\tiny U-Net-23};
        \node[align=center] at (11, 0) {\tiny U-Net-23D};
    \end{tikzpicture}
    \caption{Selected Test Set Predictions of U-Net Configurations}
    \label{fig:unet_prediction_images}
\end{figure}

The third column of the figure visualizes predictions of the \texttt{U-Net-18D} configuration. The predictions do not contain any \texttt{water} class. Instead, water bodies are predicted to be either \texttt{forest} or \texttt{buildings}. This explains why the categorical accuracy and mean IoU are lower compared to the \texttt{U-Net-23} configurations.

Both \texttt{U-Net-23} and \texttt{U-Net-23D} show better results for the \texttt{water} class. They are able to detect water surfaces correctly. However, sometimes shadows of other objects are also considered to be \texttt{water} (middle row). The first row shows that in some cases the predictions are even better than the labels.

All configurations struggle with the \texttt{traffic} class. In most situations, roads are incorrectly projected to \texttt{buildings}. Only \texttt{U-Net-23} has some correct predictions for the \texttt{traffic} class.

Based on the images shown in figure~\ref{fig:unet_prediction_images}, it is not possible to analyze the predictions with regards to the distinction between \texttt{forest} and \texttt{agriculture}. This will be considered later on in section~\ref{sec:segmentation_discussion}.

\subsection{Experiments with FC-DenseNet}
\label{sec:densenet_experiments}
In the next step, the FC-DenseNet reference architecture is investigated with regards to the segmentation challenge. Again, we analyze the original architecture like described in section~\ref{sec:densenet} as well as some variations with slightly modified hyperparameters.

For this architecture the dimensions of input and output are equally set to $224\times 224$. Since the convolution layers in this architecture use padding, the dimensions are independent from the number of layers.

The loss is again calculated by a categorical crossentropy function. During training, the weight updates are applied using a RMSprop~\cite{rmsprop14} optimizer. The learning rate is initialized with $0.001$ (according to~\cite{denseseg17}) and decreased after each epoch.

Another naming convention is applied to distinguish the variations of the FC-DenseNet models. The reference model is called \texttt{FCDN-103D}, again indicating the number of convolution layers and the use of dropout. The letter \texttt{C} denotes the application of compression layers before transition-down layers to condense the outputs of dense blocks (see~\cite{denseseg17} for details). The compression rate of those layers is set to $0.5$, i.~e. the number of feature maps is halved.

\subsubsection{Metrics}
The training metrics for all FC-DenseNet configurations are presented in figure~\ref{fig:densenet_train_metrics}. Although there are some strong oscillations, most of the configurations show ongoing improvements over the course of 20 epochs. Towards the end of the training the oscillations faint and the graphs converge.

\begin{figure}[h]
    \newcommand{\DensenetMetricsImageWidth}{0.32\textwidth}
    \centering
            \input{content/tikz/densenet_metrics}
    \caption{Validation Set Metrics of FC-DenseNet Configurations}
    \label{fig:densenet_train_metrics}
\end{figure}

This behavior is a sign that the initial learning rate was set too high. The weights in the model are then adjusted too drastically, so that minima of the loss function are skipped. The learning rate shrinks towards the end of the training, which explains the reduced oscillations.

According to figure~\ref{fig:densenet_train_metrics}, \texttt{FCDN-103D} yields the best results. At the end of the training the metrics are at a loss of $1.17$, categorical accuracy of $62.6\%$ and mean IoU of $28.4\%$. Those numbers are substantially better than the other configurations, which probably relies on the fact that this configuration also has the highest number of parameters.

The other three configurations report fairly uniform results in terms of metrics. This is very interesting with regards to performance and computational efficiency. \texttt{FCDN-103DC} has more than twice the number of parameters than \texttt{FCDN-56D} ($5.9~\text{M}$ compared to $2.7~\text{M}$) and still many more than \texttt{FCDN-67D} ($4.0~\text{M}$). This means that the configurations with fewer parameters store their information much more efficiently.

\subsubsection{Images}
Figure~\ref{fig:densenet_prediction_images} shows some sample predictions made by the FC-DenseNet configuration. The original images and ground truth labels are included for reference. All images were taken from the test set.

\begin{figure}[h]
    \newcommand{\DensenetPredictionsImageWidth}{0.15\textwidth}
    \centering

    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/101444-image} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/101444-label} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103D/101444-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103CD/101444-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-67D/101444-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-56D/101444-prediction}

    \vspace{3mm}
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/291991-image} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/291991-label} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103D/291991-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103CD/291991-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-67D/291991-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-56D/291991-prediction}

    \vspace{3mm}
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/436897-image} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/436897-label} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103D/436897-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103CD/436897-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-67D/436897-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-56D/436897-prediction}

    \vspace{3mm}
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/626312-image} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/626312-label} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103D/626312-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103CD/626312-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-67D/626312-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-56D/626312-prediction}

    \vspace{3mm}
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/768909-image} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/768909-label} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103D/768909-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103CD/768909-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-67D/768909-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-56D/768909-prediction}

    \begin{tikzpicture}
        \node[align=center] at (0, 0) {};
        \node[align=center] at (0.8, 0) {\tiny Input};
        \node[align=center] at (3.35, 0) {\tiny Ground Truth};
        \node[align=center] at (5.95, 0) {\tiny FCDN-103D};
        \node[align=center] at (8.45, 0) {\tiny FCDN-103CD};
        \node[align=center] at (10.9, 0) {\tiny FCDN-67D};
        \node[align=center] at (13.5, 0) {\tiny FCDN-56D};
    \end{tikzpicture}
    \caption{Selected Test Set Predictions of FC-DenseNet Configurations}
    \label{fig:densenet_prediction_images}
\end{figure}

While \texttt{FCDN-103D} performed very well in the metrics, the actual results have some shortcomings. The segments are not separated clearly and form some very fragmented regions. In order to use these predictions meaningfully, they would first have to be smoothed out. The same observations apply in an attenuated form to \texttt{FCDN-56D}. The  other two configurations do not exhibit this behavior.

None of the configurations was able to predict the \texttt{traffic} class correctly. Mostly it is confused with the \texttt{buildings} class.

From a purely visual point of view \texttt{FCDN-103CD} and \texttt{FCDN-67D} deliver the best results. Especially the predictions for the \texttt{water} class seem very promising.

The differentiation between the \texttt{agriculture} and \texttt{forest} classes is noticeable. In reality, the transition between those two classes is seamless and hardly perceivable. So it is expected that the predictions are subject to minor fluctuations.

\subsection{Experiments with W-Net}
\label{sec:wnet_experiments}
Lastly, the W-Net architecture presented in section~\ref{sec:w-net} is implemented. For that, big parts of the U-Net implementation could be reused, because W-Net essentially consists of two subsequent U-Nets. In contrast to the original U-Net architecture however, the W-Net employs zero padding for the convolution layers, so that input and output dimensions are equal.

The W-Net architecture was designed for unsupervised training. This means, the labels of the dataset are not consulted during training. Instead, the original images are used as both input and desired output. This way, the model has to learn an appropriate distribution of classes all by itself. Since this setup allows for arbitrary numbers of classes, the number is appended to the model name as a suffix. Otherwise, the naming convention is the same as for U-Net in section~\ref{sec:unet_experiments}.

Unlike the original architecture proposed in~\cite{wnet17} the training function was not split in two parts. Instead, the output of the encoder was altered to represent a clean one-hot encoding of the class with the highest prediction. This was done for two reasons.

First, that way the decoder only has $n$ distinct values from which it has to reconstruct the image (where $n$ is the number of classes). Thus, it is not possible to use intermediate vectors to encode additional states. This makes image reconstruction for the decoder a lot harder.

Additionally, the classes predicted by the encoder have to be picked very carefully in order to allow any meaningful reconstruction of the images at all. The goal is that the encoder is really sophisticated with regards to class predictions.

As a part of the thesis, many different options were explored for the optimizer and loss function to use for training. In the end, the results were all equally weak. The configurations presented in the next sections were trained using an Adam optimizer with a learning rate of $0.001$ and a categorical crossentropy loss function.

\subsubsection{Metrics}
The metrics for W-Net are different from the ones presented for U-Net and FC-DenseNet. Since no labels were used during training, it is not possible to calculate the categorical accuracy and mean IoU. This is because the class predictions learned by the encoder do not necessarily match with the predefined classes in the labels.

\begin{figure}[h]
    \centering
            \input{content/tikz/wnet_metrics}
    \caption{Training Set Metrics of W-Net Configurations}
    \label{fig:wnet_train_metrics}
\end{figure}

The only metric shown in figure~\ref{fig:wnet_train_metrics} is the categorical crossentropy loss during training. Since no configuration has major improvements (note the small range on the y-axis), training is stopped after a single epoch. Thus, the graph plots the loss per batch in the training set.

All configurations show similar behavior during the training. Over the first few batches of data, there is an adequate improvement on the loss. Only after a few more batches, the predictions become worse again. Interestingly, the graphs for all configurations look like they all follow the same pattern. At the time of writing, no satisfying explanation can be given for those incidents.

\subsubsection{Images}
Figure~\ref{fig:wnet_prediction_images} lists some visual results of the W-Net configurations extracted from the test set. For each configuration two images are displayed. The first one is the predicted class segmentation. The class colors in the predictions are not related to any of the classes mentioned earlier. They rather represent what the respective encoder learned to differentiate in the images. The second image is the reconstruction of the original image predicted by the decoder from the class segmentation.

\begin{figure}[h]
    \newcommand{\WnetPredictionsImageWidth}{0.10\textwidth}
    \centering

    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/original/19743-image} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/36d-6/19743-prediction} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/36d-6/19743-restored} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/46d-6/19743-prediction} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/46d-6/19743-restored} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/36d-20/19743-prediction} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/36d-20/19743-restored} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/46d-20/19743-prediction} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/46d-20/19743-restored}

    \vspace{2mm}
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/original/63585-image} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/36d-6/63585-prediction} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/36d-6/63585-restored} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/46d-6/63585-prediction} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/46d-6/63585-restored} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/36d-20/63585-prediction} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/36d-20/63585-restored} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/46d-20/63585-prediction} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/46d-20/63585-restored}

    \vspace{2mm}
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/original/78905-image} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/36d-6/78905-prediction} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/36d-6/78905-restored} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/46d-6/78905-prediction} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/46d-6/78905-restored} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/36d-20/78905-prediction} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/36d-20/78905-restored} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/46d-20/78905-prediction} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/46d-20/78905-restored}

    \vspace{2mm}
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/original/102574-image} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/36d-6/102574-prediction} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/36d-6/102574-restored} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/46d-6/102574-prediction} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/46d-6/102574-restored} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/36d-20/102574-prediction} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/36d-20/102574-restored} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/46d-20/102574-prediction} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/46d-20/102574-restored}

    \vspace{2mm}
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/original/224817-image} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/36d-6/224817-prediction} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/36d-6/224817-restored} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/46d-6/224817-prediction} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/46d-6/224817-restored} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/36d-20/224817-prediction} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/36d-20/224817-restored} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/46d-20/224817-prediction} \hfill
    \includegraphics[width=\WnetPredictionsImageWidth]{images/wnet/46d-20/224817-restored}

    \begin{tikzpicture}
        \node[align=center] at (-4, 0) {\tiny Input};
        \node[align=center] at (-1.5, 0) {\tiny W-Net-36D-6};
        \node[align=center] at (1.9, 0) {\tiny W-Net-46D-6};
        \node[align=center] at (5.3, 0) {\tiny W-Net-36D-20};
        \node[align=center] at (8.8, 0) {\tiny W-Net-46D-20};
    \end{tikzpicture}
    \caption{Selected Test Set Predictions of W-Net Configurations}
    \label{fig:wnet_prediction_images}
\end{figure}

It is clearly visible that most configurations did not learn a broad variety of classes. The predictions show mostly one dominant class and sporadic occurances of other classes. Especially for the configurations with 20 available classes this is surprising. They do not make use of the available feature space to encode more information of the original image. This is probably the reason why those configuration also were not able to reconstruct the original images properly.

For the configurations with 6 available classes, the reconstructions look much better. Especially \texttt{W-Net-46D-6} reassembles the rough structure of the original image. This is also the only configuration that makes notable use of at least two of the available classes. But still, four classes remain mostly unused.

In summary, the results of W-Net were very disappointing. Before any of the results could be used to solve the real segmentation challenge, the output would have to be post-processed. This includes e.~g. mapping the identified classes to the predefined ones. Due to the poor segmentation results this step was omitted.

\subsection{Discussion}
\label{sec:segmentation_discussion}
The W-Net reference architecture turned out to perform very poorly. With the unsupervised training, the goal was to come up with segmentation classes that were not bound to any human preconceptions. This should also help to gather more insight into the dataset in order to uncover potential improvements. However, the W-Net models did not learn any clear differentiation between classes and did not even consume the whole feature space that they were assigned. For those reasons, the W-Net approach was not very effective and is not pursued any further throughout the thesis.

The results of both U-Net and FC-DenseNet were much better in general. The experiments showed that both reference architectures are capable to provide decent predictions. Looking at the plain metrics, there is still some space for improvement in both of them. However, this could also be affected by the inaccuracies in the labels (see section~\ref{sec:dataset_considerations}). The visual results show that the predictions in some cases are even more accurate than the labels.

\begin{figure}[h]
    \newcommand{\DiscussionImageWidth}{0.23\textwidth}
    \centering

    \includegraphics[width=\DiscussionImageWidth]{images/segmentation_discussion/images/1.png} \hfill
    \includegraphics[width=\DiscussionImageWidth]{images/segmentation_discussion/labels/1.png} \hfill
    \includegraphics[width=\DiscussionImageWidth]{images/segmentation_discussion/unet/1.png} \hfill
    \includegraphics[width=\DiscussionImageWidth]{images/segmentation_discussion/densenet/1.png}

    \vspace{2mm}
    \includegraphics[width=\DiscussionImageWidth]{images/segmentation_discussion/images/2.png} \hfill
    \includegraphics[width=\DiscussionImageWidth]{images/segmentation_discussion/labels/2.png} \hfill
    \includegraphics[width=\DiscussionImageWidth]{images/segmentation_discussion/unet/2.png} \hfill
    \includegraphics[width=\DiscussionImageWidth]{images/segmentation_discussion/densenet/2.png}

    \vspace{2mm}
    \includegraphics[width=\DiscussionImageWidth]{images/segmentation_discussion/images/3.png} \hfill
    \includegraphics[width=\DiscussionImageWidth]{images/segmentation_discussion/labels/3.png} \hfill
    \includegraphics[width=\DiscussionImageWidth]{images/segmentation_discussion/unet/3.png} \hfill
    \includegraphics[width=\DiscussionImageWidth]{images/segmentation_discussion/densenet/3.png}

    \begin{tikzpicture}
        \node[align=center] at (0, 0) {};
        \node[align=center] at (0.5, 0) {\tiny Input};
        \node[align=center] at (4.4, 0) {\tiny Label};
        \node[align=center] at (8.3, 0) {\tiny U-Net-23D};
        \node[align=center] at (12, 0) {\tiny FCDN-67D};
    \end{tikzpicture}
    \caption{Large Scale Predictions of \texttt{FC-DenseNet-67D} and \texttt{U-Net-23D}}
    \label{fig:segmentation_discussion}
\end{figure}

So far, only very small areas have been looked at. In a more practical use case, the predictions would cover huge areas. Hence, figure~\ref{fig:segmentation_discussion} depict the predictions of larger areas, where each of the images covers around $580\times 580$ meters. It shows the original image and label, as well as the visualized predictions of \texttt{U-Net-23D} and \texttt{FCDN-67D}. Those configurations were chosen because they returned the best results in their individual reference architecture (see sections~\ref{sec:unet_experiments} and~\ref{sec:densenet_experiments}).

The similarities between the labels and \texttt{U-Net-23D} predictions are clearly visible. The general outlines of the class segments show only minor differences compared to the labels. This is especially true for the classes \texttt{buildings}, \texttt{forest} and \texttt{agriculture}, which reflects the expected behavior. Since these are the three dominant classes throughout the dataset, lots of good training examples are available.

Also, \texttt{U-Net-23D} provides very accurate predictions for the \texttt{water} class. In contrast to the labels, even small islands in the river are detected correctly. However, there are some false positives for this class when it comes to highly saturated green grass, such as on sports fields.

The other two classes, namely \texttt{traffic} and \texttt{urban greens}, are not predicted properly by \texttt{U-Net-23D}. For both classes this matches the expectations, since they were labelled rather poorly in the dataset. Most roads and traffic areas were not even labelled as \texttt{traffic} class, which makes it almost impossible for the models to learn the correlations correctly. For \texttt{urban greens} the reasoning is similar. Since this class mostly includes parks and sports facilities, it is easily confused with the \texttt{agriculture} or \texttt{buildings} classes. This is exactly what happens in the model's predictions.

With regards to the classes, the predictions of \texttt{FCDN-67D} can be explained in the same way. However, there is one major difference in the predictions of the two models: \texttt{FCDN-67D} is much more sensitive to fine-grained segments. Especially for the \texttt{buildings} class this can be observed. \texttt{FCDN-67D} distinguishes between single houses and the green spaces between them. In this aspect the predictions are even better than the labels in the dataset.

On the other hand, the predictions of \texttt{FCDN-67D} seem very fragmented in some cases, for example between the \texttt{agriculture} and \texttt{forest} classes. The transitions between those two are not really sharp but rather jagged and fissured. To some extent this represents the real situation, because oftentimes there are no clear borders between forest and grassland.

In terms of inference speed \texttt{U-Net-23D} is slightly faster than \texttt{FCDN-67D}. Each image in figure~\ref{fig:segmentation_discussion} took around $9$~seconds to process with \texttt{U-Net-23D}. In contrast, \texttt{FCDN-67D} took around $11$~seconds per image. All measurements were executed on a NVIDIA GeForce GTX 1080 Ti graphics card\footnote{More details on the hardware can be found in appendix~\ref{app:tools_hardware}.}.

This is surprising, because \texttt{U-Net-23D} has more than four times the number of parameters compared to \texttt{FCDN-67D} ($16.6~\text{M}$ vs.\ $4.0~\text{M}$). With \texttt{U-Net-23D} however, the parameters are spread across only $23$ convolution layers, whereas in \texttt{FCDN-67D} there are $67$ layers. Therefore, the calculations in \texttt{U-Net-23D} can better be executed in parallel.

In total, both configurations yield remarkable results for the segmentation challenge. Since the predictions of \texttt{U-Net-23D} are less fractured, it might be easier to find suitable emergency landing fields in the huge contiguous areas. However, the predictions might lack precise information of rather small objects. In that regard, the predictions from \texttt{FCDN-67D} are more beneficial.

\subsection{Deployment}
The two models \texttt{U-Net-23D} and \texttt{FCDN-67D} are also provided as Docker Images in~\cite{thesis-code20}. The latest versions of the images can also be found in the public Docker Hub repository \texttt{imageseg/terrain-segmentation}\footnote{Available at \url{https://hub.docker.com/r/imageseg/terrain-segmentation}.}. The images bundle together all dependencies like the Python interpreter, required Python packages and also the models with the trained weights. This makes it easy to use the models for inference.

The functionality of the images was deliberately kept simple, as they are for demonstration purposes only. The input consists of PNG files with RGB color scheme. The output is also rendered in PNG files, where the class segments are color-encoded as explained in section~\ref{sec:prepare_labels}. All pre- and postprocessing steps are performed automatically inside the Docker container.

For this process, two directories have to be mounted when running a container from the Docker image. The \texttt{/images} directory should contain all input images. All PNG files in this directory will be fed into the model once the container is ready. The predicted results are then written to the \texttt{/predictions} directory. For each input file, a corresponding output file with the same name is created. The container is shut down as soon as all given images are processed.

This workflow is for demonstration purposes only and is not optimized for production use. Starting and stopping Docker containers is a big overhead, because TensorFlow needs some time to ramp up and load the models. For that reason, other options have to be evaluated for production use of the models, like e.~g. TensorFlow Serving\footnote{Available at \url{https://www.tensorflow.org/tfx/guide/serving}.}. This is a production-ready system for serving TensorFlow models with a standardized interface.

However, the interface of TensorFlow Serving only works with tensor objects. Although the interface can be used directly, it makes sense to embed it in a pipeline. That allows to perform pre- and postprocessing steps independently from the model inference. The pipeline can then be connected to other system, e.~g. a PostgreSQL database. This enables the efficient processing of even large volumes of data.

\clearpage
