\section{Data Analysis}
\WIP{
data set is provided by the Ministry of the Interior of the State of North Rhine-Westphalia in Germany. they host a platform called GEOportal, accessible in \url{https://www.geoportal.nrw/}. they have all kinds of data like topographical maps, elevation information or orthographic footage. all of it is available for download and any use is permitted according to the dl-de/zero-2-0 licence (Data licence Germany, Any use is permitted without restrictions or conditions, see full text on \url{https://www.govdata.de/dl-de/zero-2-0})
}

\subsection{Getting the dataset}
\WIP{
for the thesis we need colored DOP footage. Various ways to get the data. they provide a Web Map Tile Service for DOPs, accessible on \url{https://www.wmts.nrw.de/geobasis/wmts_nw_dop}. Can be viewed with geographic information system (GIS) applications, for example QGIS. For easy access they also provide an online viewer on \url{https://www.tim-online.nrw.de/tim-online2/}.

% TODO: what is a DOP? what is NIR data?
There is a separate download section where you can choose the area and products you need and download as whole bundle. For the DOPs they use the JPEG2000 data format. this allows for high compression resulting in download size of about 11,6 GB. Provide the data in tiles, meaning there are several files each containing the data for one square tile. tile contain georeferencing information, meaning GIS applications can show the DOP at the correct position on earth's surface. Files contain 4 raster bands, RGB for color + one band with near-infrared spectral measurement (NIR) data. this is great, because we also need that later on.

\begin{itemize}
    \item discuss and compare the data formats that were offered
    \item explain the available map types and how they could be used for the thesis
    \item describe the download script
    \item show some example images from the dataset
    \item demonstrate the limits of practical feasibility
\end{itemize}
}

\subsection{Preparing the dataset}
\WIP{
first step: import data into postGIS database. allows for easy access in different tile sizes and data formats. also to have all data in one single source system. PostGIS has import tools, but don't support JPEG 2000. one intermediate step needed with GDAL. GDAL (Geospatial Data Abstraction Library) is a tool to convert and process various geospatial data formats. Supports JPEG 2000 and allows to transform to GeoTiff data format. Tiff is great data format, because it allows for extensions with metadata. in case of GeoTiff it contains georeferencing information. File can still be read by all applications supporting baseline Tiff format. PostGIS has tools to import GeoTiff as raster. Using a bash script with CLI tool \texttt{gdal\_translate} to convert file format. See listing~\ref{lst:jp2_to_tif}.

script for import is in Appendix~\ref{app:code} listing~\ref{lst:tif_to_raster}. it assumes database is already created and postgis extension enabled for that database. first creates two separate tables for image RGB raster and NIR values raster. Tiff files contain both information, RGB is in raster band 1-3, NIR is in raster band 4. But later on information is used for different purposes, so splitting does make sense.

after tables are created, loop through all tiff files and call \texttt{raster2pgsql} cli tool. it generates the sql insert statement that we need to import the data. So we pipe the output directly into \texttt{psql} to import the data. raster2pgsql gets a lot of parameters to configure everything properly. besides name of file (parameter -F) and database table most important things are tile size and spatial reference id (SRID).

tile size is set to 1000x1000, doesn't really matter to work, but major impact on performance. This size was found to be suitable for both initial import and later processing of raster.
SRID is important for geographical placement of raster. There are different reference systems, each providing a way to project earth's surface onto two-dimensional plane. The provided dataset uses SRID 25832, which is very common for data in central europe.

After all tiles are imported, an index is created. Also very important for performance reasons later on. Not a normal index, but a spatial one. Reduces computational complexity while working with raster tiles (meaning merging, intersecting, etc). Also for convenience reasons, there are constraints enforced on raster column. make sure the raster is properly aligned, has fixed extents, etc.

Last step of this script is to create a separate table containing one big polygon that defines extents of the raster. Quick way to access the raster's extents without processing the raster tiles all the time. Will be used later on for generating tiles to export as training data.

% TODO: show some images and explain the dimensions (1px = 1dm)
}

\subsection{Preparing the labels}
\WIP{
for supervised training you need labels that determine the ground truth. usually acquired by hand with a lot of manual work. to ease this process, we use predefined labels. GEOportal has a Digital Basic Landscape Model (dt. Digitales Basis-Landschaftsmodell). this is a data set that describes topographical features of landscape in vector data format. also available for download and any use permitted within dl-de/zero-2-0 license. \cite{base-dlm20}

bundle contains various shape files with many kinds of information. also has point and line data sets that represent some interesting features. we are only interested in files that contain shapes. contains much more categories than we need, so generalize some of the categories. mapping of inputs can be found in later table.
% TODO: insert table with mapping of file content, object id to segmentation category

also import the segmentation data into postgres to have it easily accessible and be able to export tiles later on. For importing there is a cli utility called \texttt{shp2pgsql}. also some arguments passed to that. file name and srid just like above. output is piped directly to \texttt{psql} to execute the statements and import the data. some intermediate steps necessary to preprocess the data.
% TODO: create shp2pgsql arguments

first, each shape file imported into separate tables, just to get the data into the database. contains much more metadata depending on the file and its contents. next step is join the data into a single table only keeping the information needed. meaning only shape itself, object id and description. since the downloaded packages contains data for the whole State of North Rhine-Westphalia, we can also drop a lot of shapes. Only shapes that intersect with the raster extents are inserted in this step. Also cut along the raster extents, so now we only have the shape that is inside the raster's extents.

Last step is to map the shape's object ids to the corresponding segmentation category. this happens according to table XYZ (see above). Also, all shapes of one category are merged into one multipolygon, meaning the final segmentation table only contains 6 geometry shapes.

\begin{itemize}
    \item raise general ideas how the labels can be represented for training/visualization
\end{itemize}
}

\subsection{Considerations about the data set}
\WIP{
shapes in the labels are not super accurate. on macroscopic point of view, they cover all segments on large areas. but borders between segments are oftentimes poorly represented. meaning transition between forest and agriculture or coastal line along rivers modelled very roughly. to get really good training results one should preferably use pixel-perfect labels. for purpose of this thesis, where the goal is to identify suitable emergency landing areas, this might not be a big issue as we are looking for open wide areas and don't care to much about perfect borders. But anyway, keep it in mind during model training.
% TODO: show images/labels to support this argument

Another thing is distribution of segmentation classes in the data set. table~\ref{tab:seg-breakdown} shows relative coverage of the area with the categories. you can see that around 65\% of the data set consist of forest. On the other hand the area of traffic and water combined is still under 1\%. In general, this is really bad for training DNNs. Will lead the network to predicting forest way more often than water/traffic. For optimal results the categories should be well balanced inside the training dataset. So this is also a point to consider later on.
% TODO: find a paper to support this argument
}

\subsection{Choose the test data set}
\WIP{

}

\begin{table}[h]
\centering
\begin{tabular}{|l|r|r|r|r|r|}
\hline
\multicolumn{1}{|c|}{\textbf{segment}} &
  \multicolumn{1}{c|}{\textbf{total $m^2$}} &
  \multicolumn{1}{c|}{\textbf{total rel. $m^2$}} &
  \multicolumn{1}{c|}{\textbf{test $m^2$}} &
  \multicolumn{1}{c|}{\textbf{test rel. $m^2$}} &
  \multicolumn{1}{c|}{\textbf{test / total}} \\ \hline
forest       & 162,554,698 & 65.40\% & 17,430,019 & 61.63\% & 10.72\% \\ \hline
buildings    & 30,821,514  & 12.40\% & 3,827,202  & 13.53\% & 12.42\% \\ \hline
urban greens & 5,715,026   & 2.30\%  & 716,624    & 2.53\%  & 12.54\% \\ \hline
agriculture  & 47,331,698  & 19.04\% & 5,898,601  & 20.86\% & 12.46\% \\ \hline
water        & 1,344,467   & 0.54\%  & 273,114    & 0.97\%  & 20.31\% \\ \hline
traffic      & 788,878     & 0.32\%  & 135,977    & 0.48\%  & 17.24\% \\ \hline
\end{tabular}
\caption{Breakdown of segments}
\label{tab:seg-breakdown}
\end{table}

\newpage
