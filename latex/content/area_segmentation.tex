\section{Segmentation of land zones}
\label{sec:segmentation}

Now that all fundamentals are covered, this chapter focuses on the implementation of the neural networks. Each reference architecture from section~\ref{sec:ref_archs} will be trained and evaluated with the original hyperparameters first. Afterwards, slight modifications are applied to the architectures, to look for optimization opportunities for the task at hand.

All implementation are done in Python with the popular Keras framework backed by TensorFlow~\cite{tf_whitepaper15}. It is a well established framework with extensive tooling, lots of documentation and a great open-source community. Also it is proven to be production-ready by a broad variety of companies for all kinds of tasks\footnote{Case studies can be found at \url{https://www.tensorflow.org/about/case-studies}}. More information about tools and hardware used throughout this thesis is listed in appendix~\ref{app:tools_hardware}

\subsection{Preparing the training and test dataset}
\label{sec:prepare_train_test}
\WIP{
\cite{DLbook16}[p.~101f]
before training we have to think about how to evaluate the models afterwards. goal is to have a model that is able to generalize, meaning it should work on data that is has not seen during training. Therefore, dataset is split into two disjoint sets of data, one called the \emph{training set} and the other called \emph{test set}. During training, only data from the training set is presented to the model. afterwards, performance is measured against the test set. That way the metrics provide a more objective view on the performance.

\cite{DLbook16}[p.~119]
test set should not be used to make any choices about the model. To optimize hyperparameters for the model, another split of training data, called \emph{validation set} is introduced. Training set is used to learn parameters, validation set is used to estimate generalization error without consulting the test set. Only after hyperparameters and model weights are optimized all the way, test set is used to measure overall performance of the model.

\cite{val_split18}
There are many ways to pick data for test and validation set. An insight into common techniques is provided by \cite{val_split18}. For this thesis, test set is selected based on imbalance between categories. For each category choose $10\%$ of samples randomly and add them to test set. This is to achieve fair results taking into account all categories accordingly. Validation set also $10\%$ of training set, but chosen randomly over all samples without considering segmentation categories.

as said in \ref{sec:dataset_considerations}, big imbalance between categories in the dataset. this might lead to unwanted biases while training the model. for examples, if network predicts only forest all the time, accuracy would be around $65\%$ measured over entire dataset. plain number is not too bad, but obviously segmentation predictions would be useless.

there are some techniques to tackle those issues. one approach is to add class weights for calculating the loss function. by doing that, incorrect predictions for some classes can be punished more than others. this will result in the model being optimized in a different way, reducing the predictions for the over-represented classes. However, this was not found to work great with given dataset. all experiments had the models end up to only predict a single class for every pixel.

Another approach was selected as follows. dataset contains huge segments that all belong to same class. it was expected, that this circumstance leads to the models learning that most examples only contain one class instead of learning the differences between classes. because of that, the dataset used for training and validation was reduced by a great margin. Basically, all image files that consist of only one class for every pixel are dropped. on the one hand, this affects class imbalance, because large partitions of forests and agricultural regions are dropped, but mostly small streets and rivers are rather kept. on the other hand, this creates incentives for the models to learn the differences between classes.

% TODO: add table with category distribution of multisegment
}

\subsection{Experiments with U-Net}

\WIP{
U-Net was first architecture to explore and experiment with. Model based on findings of \cite{unet15} and explained in detail in section \ref{sec:unet}. Keras implementation used for this thesis listed in appendix~\ref{app:code}.

input image size was chosen as $572\times 572$ pixels, same as authors used. because no padding used in convolutional layers, label/prediction size is smaller, $388\times 388$ pixels for standard U-Net architecture. architecture was also evaluated with fewer/more layers, label size respectively larger/smaller.

all models trained with stochastic gradient descent optimizer with learning rate of $0.1$ and momentum of $0.99$. learning rate is reduced after every epoch to finetune model towards the end of training. Loss function was set to categorical crossentropy.

based on original model a few other configurations have been explored. base model called \texttt{U-Net-23D}. Number in the name is number of convolution layers in the model. \texttt{D} at the end if dropout layer is used for regularization. one model trained with class weights, this has \texttt{CW} at the end of name.

\ref{fig:unet_train_metrics} shows training metrics from unet models. plots are loss, categorical accuracy, mean iou per epoch calculated from validation set during training. validation data was not used for updating weights, only for evaluation. thus, the numbers provide good evidence of models generalization ability.

clearly visible that both \texttt{U-Net-23D-CW} and \texttt{U-Net-28D} did not perform well. Training was stopped after 5 epochs, because no improvement. loss for \texttt{U-Net-23D-CW} was lower than other models, this is natural, because weigts reduce the loss for some classes, but does not increase it for the others. also other metrics show that this model performed very poorly. investigation showed that \texttt{U-Net-23D-CW} only predicts forest for all pixels. \texttt{U-Net-28D} only predicts agriculture.
% TODO: validate cat. accuracy for both. 28D seems OK, but 23D-CW seems too low for forest

for \texttt{U-Net-18D} first few epochs show improvement on all metrics. level out after around 5 epochs, then only minor changes. could be that the model reached its capacity. fewer conv layers means less parameters and thus reduced ability to learn and generalize features. end up with $61\%$ categorical accuracy and $21.7\%$ mean IoU.

best results yield model with full $23$ convolution layers. with dropout, model learns a bit faster. after 20 epochs both variants arrive at around $74\%$ categorical accuracy and $42.2\%$ mean iou.

\ref{fig:unet_prediction_images} shows model predictions in test set compared to original image and ground truth. Because of U-Net architecture, label covers smaller area than original image. red border in original image indicates the area that label covers. As discussed in section \ref{sec:dataset_considerations}, labels are not super accuracy.

For \texttt{U-Net-18D} you can see that predictions ignore water. Also roads are identified as buildings instead. reduced number of parameters are not enough to make accurate predictions for those.

\texttt{U-Net-23D} and \texttt{U-Net-23} show much better results, predictions very similar to each other. first row shows that inaccurate labels are okay, model are able to abstract away the inaccurate borders. compared to original image, predictions represent riverbanks way better than ground truth.

second row shows that both models also have trouble with traffic class. both detect the road that is not described in ground truth, but label it as buildings instead of traffic.
}

\begin{figure}
    \newcommand{\UnetMetricsImageWidth}{0.32\textwidth}
    \centering
            \input{content/tikz/unet_metrics}
    \caption{Training metrics of different variations of U-Net}
    \label{fig:unet_train_metrics}
\end{figure}

\begin{figure}
    \newcommand{\UnetPredictionsImageWidth}{0.18\textwidth}
    \centering

    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/82607-image} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/82607-label} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-18D/82607-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23/82607-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23D/82607-prediction}

    \vspace{3mm}
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/104483-image} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/104483-label} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-18D/104483-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23/104483-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23D/104483-prediction}

    \vspace{3mm}
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/114975-image} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/114975-label} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-18D/114975-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23/114975-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23D/114975-prediction}

    \vspace{3mm}
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/138625-image} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/138625-label} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-18D/138625-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23/138625-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23D/138625-prediction}

    \vspace{3mm}
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/147352-image} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/147352-label} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-18D/147352-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23/147352-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23D/147352-prediction}

    \begin{tikzpicture}
        \node[align=center] at (-1.2, 0) {\tiny Input};
        \node[align=center] at (2.0, 0) {\tiny Ground Truth};
        \node[align=center] at (5.0, 0) {\tiny U-Net-18D};
        \node[align=center] at (8.1, 0) {\tiny U-Net-23};
        \node[align=center] at (11, 0) {\tiny U-Net-23D};
    \end{tikzpicture}
    \caption{Selected predictions of different variantions of U-Net}
    \label{fig:unet_prediction_images}
\end{figure}





\subsection{Experiments with FC-DenseNet}
\WIP{
next look at results of FC-DenseNet as introduced in \ref{sec:densenet}. Also slight variations of architecture explored. also categorical crossentropy loss, but RMSProp optimizer instead of SGD. Also initial learning rate ($0.001$) lower as U-Net, as proposed by authors in \cite{denseseg17}.

Original model named \texttt{FC-DenseNet-103D} with $103$ convolution layers and dropout for regularization. Two other architectures proposed in \cite{denseseg17} with fewer convolution layers, $67$ and $56$ respectively. Also one model with compression layers before each transition down layer to condense input as in \cite{densenet18}. Compression was set to $0.5$, meaning number of feature maps is halved before transition down layer. Model has additional C in name to indicate compression.

according to metrics presented in \ref{fig:densenet_train_metrics} best model is \texttt{FC-DenseNet-103D}. Best results for all three metrics with loss of $1.17$, categorical accuracy of $62.6\%$ and mean IoU of $28.4\%$. Other variants have fewer parameters, so less capacity to learn generalization.

All models show some oscillations during middle epochs of training. Towards end of training at epoch 20, oscillation is reduced but not gone. this may come from learning rate being too high and model overshooting minimums in loss function. since learning rate is reduced after each epoch, this explains convergence towards the end.

taking into account the practical usability, it is usefull to look at actual predictions. figure \ref{fig:densenet_prediction_images} shows original image, ground truth and predictions of the models. \texttt{FC-DenseNet-103D}
 predictions are very fragmented for some regions. this indicates that confidence for predictions is rather low. other models do not show so much fragmentation.

From a visual aspect, \texttt{FC-DenseNet-103CD} and \texttt{FC-DenseNet-67D} seem to deliver best results. especially predictions for water are rather nice. However, predictions for traffic are very bad. Not a single model delivers predictions for traffic, all of them confuse it with buildings.
}

\begin{figure}
    \newcommand{\DensenetMetricsImageWidth}{0.32\textwidth}
    \centering
            \input{content/tikz/densenet_metrics}
    \caption{Training metrics of different variations of FC-DenseNet}
    \label{fig:densenet_train_metrics}
\end{figure}

\begin{figure}
    \newcommand{\DensenetPredictionsImageWidth}{0.15\textwidth}
    \centering

    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/101444-image} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/101444-label} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103D/101444-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103CD/101444-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-67D/101444-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-56D/101444-prediction}

    \vspace{3mm}
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/291991-image} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/291991-label} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103D/291991-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103CD/291991-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-67D/291991-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-56D/291991-prediction}

    \vspace{3mm}
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/436897-image} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/436897-label} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103D/436897-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103CD/436897-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-67D/436897-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-56D/436897-prediction}

    \vspace{3mm}
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/626312-image} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/626312-label} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103D/626312-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103CD/626312-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-67D/626312-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-56D/626312-prediction}

    \vspace{3mm}
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/768909-image} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/768909-label} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103D/768909-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103CD/768909-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-67D/768909-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-56D/768909-prediction}

    \begin{tikzpicture}
        \node[align=center] at (0, 0) {};
        \node[align=center] at (0.8, 0) {\tiny Input};
        \node[align=center] at (3.35, 0) {\tiny Ground Truth};
        \node[align=center] at (5.95, 0) {\tiny DenseNet-103D};
        \node[align=center] at (8.45, 0) {\tiny DenseNet-103CD};
        \node[align=center] at (10.9, 0) {\tiny DenseNet-67D};
        \node[align=center] at (13.5, 0) {\tiny DenseNet-56D};
    \end{tikzpicture}
    \caption{Selected predictions of different variantions of FC-DenseNet}
    \label{fig:densenet_prediction_images}
\end{figure}

\subsection{Experiments with W-Net}
\WIP{
\begin{itemize}
    \item show the architectures of all W-Net models
    \item describe the idea behind the architectural decisions
\end{itemize}
}

\subsection{Discussion}
\WIP{
\begin{itemize}
    \item have a look at the segments that were found by the NN
    \item show segmentation results of large areas
    \item compare segments to the real landscape in the dataset
    \item discuss practical use of those results for identifying emergency landing fields
    \item evaluate results and assess which data to use for the vegetation analysis
\end{itemize}
}

\newpage
