\section{Data Analysis}
\WIP{
data set is provided by the Ministry of the Interior of the State of North Rhine-Westphalia in Germany. they host a platform called GEOportal, accessible in \url{https://www.geoportal.nrw/}. they have all kinds of data like topographical maps, elevation information or orthographic footage. all of it is available for download and any use is permitted according to the dl-de/zero-2-0 licence (Data licence Germany, Any use is permitted without restrictions or conditions, see full text on \url{https://www.govdata.de/dl-de/zero-2-0})
}

\subsection{Getting the dataset}
\WIP{
for the thesis we need colored DOP footage. Various ways to get the data. they provide a Web Map Tile Service for DOPs, accessible on \url{https://www.wmts.nrw.de/geobasis/wmts_nw_dop}. Can be viewed with geographic information system (GIS) applications, for example QGIS. For easy access they also provide an online viewer on \url{https://www.tim-online.nrw.de/tim-online2/}.

% TODO: what is a DOP? what is NIR data?
There is a separate download section where you can choose the area and products you need and download as whole bundle. For the DOPs they use the JPEG2000 data format. this allows for high compression resulting in download size of about 11,6 GB. Provide the data in tiles, meaning there are several files each containing the data for one square tile. tile contain georeferencing information, meaning GIS applications can show the DOP at the correct position on earth's surface. Files contain 4 raster bands, RGB for color + one band with near-infrared spectral measurement (NIR) data. this is great, because we also need that later on.

\begin{itemize}
    \item discuss and compare the data formats that were offered
    \item explain the available map types and how they could be used for the thesis
    \item describe the download script
    \item show some example images from the dataset
    \item demonstrate the limits of practical feasibility
\end{itemize}
}

\subsection{Preparing the dataset}
\WIP{
first step: import data into postGIS database. allows for easy access in different tile sizes and data formats. also to have all data in one single source system. PostGIS has import tools, but don't support JPEG 2000. one intermediate step needed with GDAL. GDAL (Geospatial Data Abstraction Library) is a tool to convert and process various geospatial data formats. Supports JPEG 2000 and allows to transform to GeoTiff data format. Tiff is great data format, because it allows for extensions with metadata. in case of GeoTiff it contains georeferencing information. File can still be read by all applications supporting baseline Tiff format. PostGIS has tools to import GeoTiff as raster. Using a bash script with CLI tool \texttt{gdal\_translate} to convert file format. See listing~\ref{lst:jp2_to_tif}.

script for import is in Appendix~\ref{app:code} listing~\ref{lst:tif_to_raster}. it assumes database is already created and postgis extension enabled for that database. first creates two separate tables for image RGB raster and NIR values raster. Tiff files contain both information, RGB is in raster band 1-3, NIR is in raster band 4. But later on information is used for different purposes, so splitting does make sense.

% TODO: check -I and -F parameters for raster2pgsql
after tables are created, loop through all tiff files and call \texttt{raster2pgsql} cli tool. it generates the sql insert statement that we need to import the data. So we pipe the output directly into \texttt{psql} to import the data. raster2pgsql gets a lot of parameters to configure everything properly. besides name of file and database table most important things are tile size and spatial reference id (SRID).

tile size is set to 1000x1000, doesn't really matter to work, but major impact on performance. This size was found to be suitable for both initial import and later processing of raster.
SRID is important for geographical placement of raster. There are different reference systems, each providing a way to project earth's surface onto two-dimensional plane. The provided dataset uses SRID 25832, which is very common for data in central europe.

After all tiles are imported, an index is created. Also very important for performance reasons later on. Not a normal index, but a spatial one. Reduces computational complexity while working with raster tiles (meaning merging, intersecting, etc). Also for convenience reasons, there are constraints enforced on raster column. make sure the raster is properly aligned, has fixed extents, etc.

Last step of this script is to create a separate table containing one big polygon that defines extents of the raster. Quick way to access the raster's extents without processing the raster tiles all the time. Will be used later on for generating tiles to export as training data.

% TODO: summarize shp_to_geom.py
}


\subsection{Preparing the labels}
\WIP{
\begin{itemize}
    \item describe download process
    \item explain transformations (import to postgres, export as image)
    \item raise general ideas how the labels can be represented for training/visualization
\end{itemize}
}

\subsection{Choose the test data set}
\WIP{

}

\begin{table}[h]
\centering
\begin{tabular}{|l|r|r|r|r|r|}
\hline
\multicolumn{1}{|c|}{\textbf{segment}} &
  \multicolumn{1}{c|}{\textbf{total $m^2$}} &
  \multicolumn{1}{c|}{\textbf{total rel. $m^2$}} &
  \multicolumn{1}{c|}{\textbf{test $m^2$}} &
  \multicolumn{1}{c|}{\textbf{test rel. $m^2$}} &
  \multicolumn{1}{c|}{\textbf{test / total}} \\ \hline
forest       & 162,554,698 & 65.40\% & 17,430,019 & 61.63\% & 10.72\% \\ \hline
buildings    & 30,821,514  & 12.40\% & 3,827,202  & 13.53\% & 12.42\% \\ \hline
urban greens & 5,715,026   & 2.30\%  & 716,624    & 2.53\%  & 12.54\% \\ \hline
agriculture  & 47,331,698  & 19.04\% & 5,898,601  & 20.86\% & 12.46\% \\ \hline
water        & 1,344,467   & 0.54\%  & 273,114    & 0.97\%  & 20.31\% \\ \hline
traffic      & 788,878     & 0.32\%  & 135,977    & 0.48\%  & 17.24\% \\ \hline
\end{tabular}
\caption{Breakdown of segments}
\label{tab:seg-breakdown}
\end{table}

\newpage
