\section{Segmentation of land zones}
\label{sec:segmentation}

Now that all fundamentals are covered, this chapter focuses on the implementation of the neural networks. Each reference architecture from section~\ref{sec:ref_archs} will be trained and evaluated with the original hyperparameters first. Afterwards, slight modifications are applied to the architectures, to look for optimization opportunities for the task at hand.

All implementation are done in Python with the popular Keras framework backed by TensorFlow~\cite{tf_whitepaper15}. It is a well established framework with extensive tooling, lots of documentation and a great open-source community. Also it is proven to be production-ready by a broad variety of companies for all kinds of tasks\footnote{Case studies can be found at \url{https://www.tensorflow.org/about/case-studies}}. More information about tools and hardware used throughout this thesis is listed in appendix~\ref{app:tools_hardware}

\subsection{Preparing the training and test dataset}
\label{sec:prepare_train_test}

Since the goal is to have a model that is able to generalize, it is recommended to measure its performance on data that it has not seen during training. Therefore, the dataset is split in two disjoint sets of data called \emph{training set} and \emph{test set}~\cite[p.~101f]{DLbook16}. During training, only samples from the training set are presented to the model. Afterwards, the performance is measured with samples from the test set. That way, models which just memorize the training examples perform rather bad compared to models which learn features on a generalized level.

The test set should only be used to evaluate the performance, but not decide on hyperparameters or architecture of the model. Otherwise, the model may become too closely aligned with the test set, i.~e. it is no longer possible to evaluate the generalization ability of the model with it. For such decisions another split of the training set is made, which is then called \emph{validation set}~\cite[p.~119]{DLbook16}. This set is used to estimate the generalization error during training and optimization without consulting the test set. Only after all optimizations have been done and no further changes to the model are planned, the test set is used to evaluate the final performance of the model.

There are many ways to pick the samples for the test and validation sets. \cite{val_split18} provides an insight into commonly used techniques. For this thesis, the test set respects the class imbalance that is found within the full dataset (see section~\ref{sec:dataset_considerations}). This is to achieve fair evaluation results which are not biased by any particular class. For each class, $10\%$ of the samples containing this class were chosen and added to the test set. The remaining samples are randomly distributed among training and validation set at a ratio of $9:1$.

\subsection{Tackling class imbalance}
\label{sec:class_imbalance}

As already expressed in section~\ref{sec:dataset_considerations}, there is a large imbalance between the segmentation classes in the dataset. This can lead to undesirable biases when training a model with such a dataset. For example, if the model predicts the \texttt{forest} class for every pixel all the time, the categorical accuracy would be at $65\%$ measured over the entire dataset. While this number does not sound too bad, the actual segmentation results would be useless.

There are some techniques to tackle the issues that come with an imbalanced dataset. One approach is to add class specific weights for the loss function during training~\cite{class_imbalance19}. The weights will affect the outcome of the loss function and therefore also the gradients during backpropagation. They are assigned antiproportionally to the distribution of the classes, i.~e. overrepresented classes are given small weights and underrepresented classes are given big weights. This has the same effect as duplicating the underrepresented samples in the training set, but it is computationally more efficient.

One major challenge is to find appropriate class weights, so that the imbalances are minimized as much as possible. For the given dataset, this can be done by taking into account the respective surface area for each class. Many different sets of class weights were tested, but none of them proved to be well suited. All trained models ended up predicting only a single class for all pixels all the time. Hence the class weights were dropped and not pursued any further.

At a closer look on the dataset, most segments are huge contiguous patches of the same class. The export of the image tiles as described in section~\ref{sec:image_export} results in a high number of images which only cover one single class. This might have a major impact on the learning outcome. Since most of the samples presented to the model only consist of a single class, it is very likely that the model aligns to this characteristics. So instead of learning to differentiate between segments with precise boundaries, the model might rather be directed to roughly classify larger areas.

For this reason, a different approach was chosen to address the class imbalance issues. Since the overall goal for the model is to differentiate between classes, images containing only a single class do not provide much value for training. Thus, all image tiles where all pixels belong to the same class are dropped entirely. One the one hand, this creates incentives for the model to better recognize the differences between classes and the characteristics of class boundaries. On the other hand, this also affects class imbalance, since large partitions of forests and agricultural areas are dropped, but mostly small streets and rivers are preserved. The first few attempts with the reduced training set looked    very promising. Hence, this strategy will be used further on for the training of all the models.

% TODO: add table with category distribution of multisegment and discuss it

\subsection{Experiments with U-Net}
The reference architecture that was explored first is the U-Net as introduced in section~\ref{sec:unet}. The architecture was implemented with the Keras framework. The code with all implementation details is listed in appendix~\ref{app:code}.

The default input size for images was set to $572\times 572$ pixels. Those are the same dimensions as the authors of U-Net used in their initial approach~\cite{unet15}. Because of the unpadded convolutional layers, the dimensions of the predictions are smaller than the original input image. For the standard configuration in U-Net the output size is $388\times 388$ pixels. Depending on the number of convolutional layers, these dimensions may vary.

All U-Net models were trained with a stochastic gradient descent optimizer. The initial learning rate was set to $0.1$ and reduced after every epoch, to allow finetuning of the weights towards the end of the training. A batch size of $4$ was used in combination with Nesterov momentum~\cite{nesterov83} of $0.99$ to speed up the training process and avoid oscillations. The loss was computed using a categorical crossentropy function.

Starting from the model with its original hyperparameter configuration, a few other configurations have been explored. To distinguish between the configurations, a naming convention is established. The original configuration is called \texttt{U-Net-23D}, where the number represents the exact number of convolutional layers in the model. The suffix \texttt{D} indicates that a dropout layer was added before the expansive path to provide regularization during the training.

\subsubsection{Metrics}
During training, some metrics were collected to measure the predictive performance of the models (see section~\ref{sec:metrics} for details on the metrics). The plots in figure~\ref{fig:unet_train_metrics} present the epoch-wise loss, categorical accuracy and mean IoU for each U-Net configuration. The computation of the metrics was performed using the validation data set. This set only contains samples that were not fed into the model for updating its weights. Thus, the metrics provide good evidence for the model's ability to generalize.

\begin{figure}
    \newcommand{\UnetMetricsImageWidth}{0.32\textwidth}
    \centering
            \input{content/tikz/unet_metrics}
    \caption{Training metrics of different variations of U-Net}
    \label{fig:unet_train_metrics}
\end{figure}

It is clearly visible that \texttt{U-Net-28D} does not perform well. For all three metrics it yields the worst results. Also, it does show any significant improvement over time, which is why the training was stopped after five epochs. On a closer investigation of the bad performance, it was found that the model always predicts the \texttt{agriculture} class for every pixel.
\WIP{Confidence for predictions? What is the root cause for this behavior? Too deep, so features are lost throughout the network?}
% TODO: calculate confidence, give reasons for this behavior

The configuration \texttt{U-Net-18D} shows some improvements regarding the metrics in the first few epochs of training. After the fifth epoch the graphs flatten out and show only minor improvements. The training was stopped after ten epochs in total, where it hits around $60.8\%$ categorical accuracy and $21.6\%$ mean IoU. The \texttt{U-Net-18D} configuration was the one with the least number of parameters. Fewer parameters directly affect the ability to learn and generalize a broad range of features. The assumption is that the capacity of the model is not sufficient to comprehend the full space of features involved in this challenge.

The best results with regards to the metrics achieve both \texttt{U-Net-23D} and \texttt{U-Net-23}. They both show similar rates of improvements, which level out after $25$ epochs. The configuration with dropout layers features a slightly faster learning behavior, but both arrive at around $74\%$ categorical accuracy and $42.2\%$ mean IoU in the end.

The metrics already allow to draw some conclusions about the configurations. However, it is useful to also look at the visual results before starting the discussion.

\subsubsection{Images}
\WIP{
\ref{fig:unet_prediction_images} shows model predictions in test set compared to original image and ground truth. Because of U-Net architecture, label covers smaller area than original image. red border in original image indicates the area that label covers. As discussed in section \ref{sec:dataset_considerations}, labels are not super accuracy.

For \texttt{U-Net-18D} you can see that predictions ignore water. Also roads are identified as buildings instead. reduced number of parameters are not enough to make accurate predictions for those.

\texttt{U-Net-23D} and \texttt{U-Net-23} show much better results, predictions very similar to each other. first row shows that inaccurate labels are okay, model are able to abstract away the inaccurate borders. compared to original image, predictions represent riverbanks way better than ground truth.

second row shows that both models also have trouble with traffic class. both detect the road that is not described in ground truth, but label it as buildings instead of traffic.
}

\begin{figure}
    \newcommand{\UnetPredictionsImageWidth}{0.18\textwidth}
    \centering

    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/82607-image} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/82607-label} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-18D/82607-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23/82607-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23D/82607-prediction}

    \vspace{3mm}
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/104483-image} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/104483-label} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-18D/104483-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23/104483-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23D/104483-prediction}

    \vspace{3mm}
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/114975-image} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/114975-label} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-18D/114975-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23/114975-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23D/114975-prediction}

    \vspace{3mm}
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/138625-image} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/138625-label} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-18D/138625-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23/138625-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23D/138625-prediction}

    \vspace{3mm}
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/147352-image} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/147352-label} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-18D/147352-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23/147352-prediction} \hfill
    \includegraphics[width=\UnetPredictionsImageWidth]{images/unet/unet-23D/147352-prediction}

    \begin{tikzpicture}
        \node[align=center] at (-1.2, 0) {\tiny Input};
        \node[align=center] at (2.0, 0) {\tiny Ground Truth};
        \node[align=center] at (5.0, 0) {\tiny U-Net-18D};
        \node[align=center] at (8.1, 0) {\tiny U-Net-23};
        \node[align=center] at (11, 0) {\tiny U-Net-23D};
    \end{tikzpicture}
    \caption{Selected predictions of different variantions of U-Net}
    \label{fig:unet_prediction_images}
\end{figure}





\subsection{Experiments with FC-DenseNet}
\WIP{
next look at results of FC-DenseNet as introduced in \ref{sec:densenet}. Also slight variations of architecture explored. also categorical crossentropy loss, but RMSProp optimizer instead of SGD. Also initial learning rate ($0.001$) lower as U-Net, as proposed by authors in \cite{denseseg17}.

Original model named \texttt{FC-DenseNet-103D} with $103$ convolution layers and dropout for regularization. Two other architectures proposed in \cite{denseseg17} with fewer convolution layers, $67$ and $56$ respectively. Also one model with compression layers before each transition down layer to condense input as in \cite{densenet18}. Compression was set to $0.5$, meaning number of feature maps is halved before transition down layer. Model has additional C in name to indicate compression.

according to metrics presented in \ref{fig:densenet_train_metrics} best model is \texttt{FC-DenseNet-103D}. Best results for all three metrics with loss of $1.17$, categorical accuracy of $62.6\%$ and mean IoU of $28.4\%$. Other variants have fewer parameters, so less capacity to learn generalization.

All models show some oscillations during middle epochs of training. Towards end of training at epoch 20, oscillation is reduced but not gone. this may come from learning rate being too high and model overshooting minimums in loss function. since learning rate is reduced after each epoch, this explains convergence towards the end.

taking into account the practical usability, it is usefull to look at actual predictions. figure \ref{fig:densenet_prediction_images} shows original image, ground truth and predictions of the models. \texttt{FC-DenseNet-103D}
 predictions are very fragmented for some regions. this indicates that confidence for predictions is rather low. other models do not show so much fragmentation.

From a visual aspect, \texttt{FC-DenseNet-103CD} and \texttt{FC-DenseNet-67D} seem to deliver best results. especially predictions for water are rather nice. However, predictions for traffic are very bad. Not a single model delivers predictions for traffic, all of them confuse it with buildings.
}

\begin{figure}
    \newcommand{\DensenetMetricsImageWidth}{0.32\textwidth}
    \centering
            \input{content/tikz/densenet_metrics}
    \caption{Training metrics of different variations of FC-DenseNet}
    \label{fig:densenet_train_metrics}
\end{figure}

\begin{figure}
    \newcommand{\DensenetPredictionsImageWidth}{0.15\textwidth}
    \centering

    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/101444-image} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/101444-label} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103D/101444-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103CD/101444-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-67D/101444-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-56D/101444-prediction}

    \vspace{3mm}
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/291991-image} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/291991-label} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103D/291991-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103CD/291991-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-67D/291991-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-56D/291991-prediction}

    \vspace{3mm}
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/436897-image} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/436897-label} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103D/436897-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103CD/436897-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-67D/436897-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-56D/436897-prediction}

    \vspace{3mm}
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/626312-image} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/626312-label} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103D/626312-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103CD/626312-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-67D/626312-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-56D/626312-prediction}

    \vspace{3mm}
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/768909-image} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/768909-label} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103D/768909-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-103CD/768909-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-67D/768909-prediction} \hfill
    \includegraphics[width=\DensenetPredictionsImageWidth]{images/densenet/densenet-56D/768909-prediction}

    \begin{tikzpicture}
        \node[align=center] at (0, 0) {};
        \node[align=center] at (0.8, 0) {\tiny Input};
        \node[align=center] at (3.35, 0) {\tiny Ground Truth};
        \node[align=center] at (5.95, 0) {\tiny DenseNet-103D};
        \node[align=center] at (8.45, 0) {\tiny DenseNet-103CD};
        \node[align=center] at (10.9, 0) {\tiny DenseNet-67D};
        \node[align=center] at (13.5, 0) {\tiny DenseNet-56D};
    \end{tikzpicture}
    \caption{Selected predictions of different variantions of FC-DenseNet}
    \label{fig:densenet_prediction_images}
\end{figure}

\subsection{Experiments with W-Net}
\WIP{
\begin{itemize}
    \item show the architectures of all W-Net models
    \item describe the idea behind the architectural decisions
\end{itemize}
}

\subsection{Discussion}
\WIP{
\begin{itemize}
    \item have a look at the segments that were found by the NN
    \item show segmentation results of large areas
    \item compare segments to the real landscape in the dataset
    \item discuss practical use of those results for identifying emergency landing fields
    \item evaluate results and assess which data to use for the vegetation analysis
\end{itemize}
}

\newpage
