\section{About the dataset}
The dataset which is used throughout this thesis is provided by the Ministry of the Interior of the State of North Rhine-Westphalia in Germany. With the platform GEOportal\footnote{available at \url{https://www.geoportal.nrw/}} they offer various types of maps, like topographical maps, elevation data and orthographical footage. Most of the data is available for free download in batches. The batches used in this thesis are released under the dl-de/zero-2-0 licence. This means that the data can be used for any purpose without any restrictions or conditions\footnote{see the full license text at \url{https://www.govdata.de/dl-de/zero-2-0}}.

For the purpose of this thesis there are two types of maps which prove useful. First, there is a map assembled of digital orthophotos (DOPs). A DOP is an aerial photograph of the surface of the earth. It is processed to hide effects like perspective distortions or topographic features of the landscape. Also it follows a specific map projection to denote the exact spatial extents of the photograph on the earth's surface. Because of that, DOPs are great to analyze terrain coverage and conditions. In section~\ref{sec:segmentation} DOPs are used to perform a semantic segmentation based on the terrain surface.

The second map explored in this thesis contains imagery obtained by near-infrared (NIR) spectroscopy. They are processed in the same way as the DOPs, so they are also projected onto the earth's surface with a specific map projection. NIR data is widely applied in agriculture to monitor the cultivation of herbal products like forages and vegetables. In this thesis the NIR imagery is used to approximate vegetation density for specific regions (see section~\ref{sec:vegetation_analysis}).

\subsection{Getting the dataset}
Both the DOP and NIR datasets are provided by the Ministry of the Interior in a few different ways. To get a quick overview, there is an online viewer for most of the map types available at \url{https://www.tim-online.nrw.de/tim-online2/}. For some specific maps like the DOPs they host a Web Map Tile Service, which allows to access the data with geographic information systems (GIS) like QGIS\footnote{QGIS is a free and open-source GIS application available at \url{https://qgis.org/}}. Since these options require a continuous network connection, it is preferred to get a local copy of the dataset and work with that.

For that purpose, the GEOportal has a separate download section. There you can choose the regions and map types you need and download them in a compressed bundle. The bundle contains map tiles in the JPEG 2000 file format. This is an image format that has a dense compression rate and directly contains the georeferencing for each tile. To have a wide range of terrain types with broad variety included, we use the data for the \WIP{Landkreise X, Y, Z}. This concludes to a download size of \WIP{XX GB} with around \WIP{YY square km} of terrain.
% TODO: which areas were selected, download size, terrain area, tile size, image resolution
% TODO: JPEG 2000 not very widely accepted

It is possible to download both DOP and NIR data together in a single bundle. The JPEG files then contain four bands of pixel information. The first three bands make up the red, green and blue colors for the DOPs. And the last band provides the scalar output of the NIR spectroscopy scan.


% TODO: show some example images from the dataset, maybe later?
% TODO: demonstrate the limits of practical feasibility, maybe later?

\subsection{Preparing the dataset}
As a first step, the whole dataset is imported into a spatial database system. By doing that, it is very easy to export the data in various data formats and tile sizes required by the reference architectures (see section~\ref{sec:ref_archs}). PostgreSQL\footnote{see \url{https://www.postgresql.org/}} is a powerful open-source database system. Together with PostGIS\footnote{see \url{https://postgis.net/}}, a free and open-source extension for PostgreSQL, it is capable of performing spatial operations on image rasters and vector objects. For example, it adds database functions to merge raster tiles, calculate intersection regions or determine bounding boxes.
% TODO: created a database with postgis as single source of truth

The PostGIS installation contains a few shell tools to import data into the database. Unfortunately, they do not support the JPEG 2000 image file format. Before the import of the data can take place, it has to be translated to one of the supported formats first. One of the most widely used formats is GeoTiff. The Tiff file standard is great, because it consists of a baseline section which contains the image information, and a meta section which can contain all kinds of meta information. For example, it can be used to store georeferencing information, which PostGIS is able to use for importing GeoTiff files.

To convert the files from JPEG 2000 to GeoTiff, we use the \emph{Geospatial Data Abstraction Library} (GDAL). This is a tool collection which acts as an abstraction layer for various geospatial data formats. It also includes a shell tool to translate several georeferenced image file formats, including JPEG 2000 and GeoTiff. Listing~\ref{lst:jp2_to_tif} in appendix~\ref{app:code} shows a bash script using the \texttt{gdal\_translate} tool to convert all the JPEG 2000 files to the GeoTiff format.

\WIP{
script for import is in Appendix~\ref{app:code} listing~\ref{lst:tif_to_raster}. it assumes database is already created and postgis extension enabled for that database. first creates two separate tables for image RGB raster and NIR values raster. Tiff files contain both information, RGB is in raster band 1-3, NIR is in raster band 4. But later on information is used for different purposes, so splitting does make sense.

after tables are created, loop through all tiff files and call \texttt{raster2pgsql} cli tool. it generates the sql insert statement that we need to import the data. So we pipe the output directly into \texttt{psql} to import the data. raster2pgsql gets a lot of parameters to configure everything properly. besides name of file (parameter -F) and database table most important things are tile size and spatial reference id (SRID).

tile size is set to 1000x1000, doesn't really matter to work, but major impact on performance. This size was found to be suitable for both initial import and later processing of raster.
SRID is important for geographical placement of raster. There are different reference systems, each providing a way to project earth's surface onto two-dimensional plane. The provided dataset uses SRID 25832, which is very common for data in central europe.

After all tiles are imported, an index is created. Also very important for performance reasons later on. Not a normal index, but a spatial one. Reduces computational complexity while working with raster tiles (meaning merging, intersecting, etc). Also for convenience reasons, there are constraints enforced on raster column. make sure the raster is properly aligned, has fixed extents, etc.

Last step of this script is to create a separate table containing one big polygon that defines extents of the raster. Quick way to access the raster's extents without processing the raster tiles all the time. Will be used later on for generating tiles to export as training data.

% TODO: show some images and explain the dimensions (1px = 1dm)
}

\subsection{Preparing the labels}
\WIP{
for supervised training you need labels that determine the ground truth. usually acquired by hand with a lot of manual work. to ease this process, we use predefined labels. GEOportal has a Digital Basic Landscape Model (dt. Digitales Basis-Landschaftsmodell). this is a data set that describes topographical features of landscape in vector data format. also available for download and any use permitted within dl-de/zero-2-0 license. \cite{base-dlm20}

bundle contains various shape files with many kinds of information. also has point and line data sets that represent some interesting features. we are only interested in files that contain shapes. contains much more categories than we need, so generalize some of the categories. mapping of inputs can be found in later table.
% TODO: insert table with mapping of file content, object id to segmentation category

also import the segmentation data into postgres to have it easily accessible and be able to export tiles later on. For importing there is a cli utility called \texttt{shp2pgsql}. also some arguments passed to that. file name and srid just like above. output is piped directly to \texttt{psql} to execute the statements and import the data. some intermediate steps necessary to preprocess the data.
% TODO: create shp2pgsql arguments

first, each shape file imported into separate tables, just to get the data into the database. contains much more metadata depending on the file and its contents. next step is join the data into a single table only keeping the information needed. meaning only shape itself, object id and description. since the downloaded packages contains data for the whole State of North Rhine-Westphalia, we can also drop a lot of shapes. Only shapes that intersect with the raster extents are inserted in this step. Also cut along the raster extents, so now we only have the shape that is inside the raster's extents.

Last step is to map the shape's object ids to the corresponding segmentation category. this happens according to table XYZ (see above). Also, all shapes of one category are merged into one multipolygon, meaning the final segmentation table only contains 6 geometry shapes.

\begin{itemize}
    \item raise general ideas how the labels can be represented for training/visualization
\end{itemize}
}

\subsection{Considerations about the data set}
\WIP{
shapes in the labels are not super accurate. on macroscopic point of view, they cover all segments on large areas. but borders between segments are oftentimes poorly represented. meaning transition between forest and agriculture or coastal line along rivers modelled very roughly. to get really good training results one should preferably use pixel-perfect labels. for purpose of this thesis, where the goal is to identify suitable emergency landing areas, this might not be a big issue as we are looking for open wide areas and don't care to much about perfect borders. But anyway, keep it in mind during model training.
% TODO: show images/labels to support this argument

Another thing is distribution of segmentation classes in the data set. table~\ref{tab:seg-breakdown} shows relative coverage of the area with the categories. you can see that around 65\% of the data set consist of forest. On the other hand the area of traffic and water combined is still under 1\%. In general, this is really bad for training DNNs. Will lead the network to predicting forest way more often than water/traffic. For optimal results the categories should be well balanced inside the training dataset. So this is also a point to consider later on.
% TODO: find a paper to support this argument
}

\subsection{Choose the test data set}
\WIP{

}

\begin{table}[h]
\centering
\begin{tabular}{|l|r|r|r|r|r|}
\hline
\multicolumn{1}{|c|}{\textbf{segment}} &
  \multicolumn{1}{c|}{\textbf{total $m^2$}} &
  \multicolumn{1}{c|}{\textbf{total rel. $m^2$}} &
  \multicolumn{1}{c|}{\textbf{test $m^2$}} &
  \multicolumn{1}{c|}{\textbf{test rel. $m^2$}} &
  \multicolumn{1}{c|}{\textbf{test / total}} \\ \hline
forest       & 162,554,698 & 65.40\% & 17,430,019 & 61.63\% & 10.72\% \\ \hline
buildings    & 30,821,514  & 12.40\% & 3,827,202  & 13.53\% & 12.42\% \\ \hline
urban greens & 5,715,026   & 2.30\%  & 716,624    & 2.53\%  & 12.54\% \\ \hline
agriculture  & 47,331,698  & 19.04\% & 5,898,601  & 20.86\% & 12.46\% \\ \hline
water        & 1,344,467   & 0.54\%  & 273,114    & 0.97\%  & 20.31\% \\ \hline
traffic      & 788,878     & 0.32\%  & 135,977    & 0.48\%  & 17.24\% \\ \hline
\end{tabular}
\caption{Breakdown of segments}
\label{tab:seg-breakdown}
\end{table}

\newpage
