\section{Literature Review}
\WIP{
\begin{itemize}
    \item explain what a neural network is in general
    \item brief summary on how the learning works (backpropagation)
\end{itemize}
}

\subsection{Deep Learning Basics}
\emph{Deep Learning} is a new field of machine learning that makes use of \emph{Deep Neural Networks} (DNN)~\cite[pp.~125f]{nn_intro96}. In an abstract point of view, a DNN is just a mathematical function to map a given input vector to an output vector. It is assembled from densely-connected units called \emph{neurons} (or \emph{perceptrons}). Usually, the neurons are grouped into layers~\cite[p.~125]{nn_intro96} (see figure~\ref{fig:layered_architecture}). The first layer of the DNN has direct access to the input vector, thus it is called the \emph{input layer}. In contrast, the last layer of the DNN produces the output vector and is therefore called \emph{output layer}. In between, there is an arbitrary number of \emph{hidden layers} that make up for the depth of the neural network.

% TODO: increase spacing above image
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{images/generic_layered_architecture}
    \caption{A generic layered DNN architecture~\cite[p.~126]{nn_intro96}}
    \label{fig:layered_architecture}
\end{figure}

Each neuron is itself a small computational unit that applies several mathematical operations, namely an \emph{input function} and an \emph{activation function}. Traditionally, the neurons of subsequent layers are densely connected, meaning a neuron of layer $k$ receives the output values from all neurons in layer $k-1$. Based on those values, the input function calculates the \emph{net input} for the neuron. In most cases this is done by using weights on the connections to perform a weighted sum over all values. The net input is then passed through the activation function whose main purpose is to break the linearity of the DNN. For the activation function there are plenty of options to choose from. An overview of commonly used activation functions can be found in~\cite{act_funcs18} and includes for example a binary step function, a sigmoid function or rectified linear units (ReLU). Figure~\ref{fig:perceptron} depicts all the calculations done inside of a neuron.

% TODO: find/create better matching illustration, or explain the figure a bit more
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{images/perceptron}
    \caption{Perceptron}
    \label{fig:perceptron}
\end{figure}

To adapt a DNN model for a specific task, the weights for all the connections have to be initialized with proper values. Since there are so many weights, it is not feasible to choose the values by hand, but rather use a process called \emph{training}. To train the DNN model, it is fed with examples of input vectors with their respective output vectors. During this process the weights are adjusted slightly, so that the model better reflects the relations between the input and the output vectors. This is a computationally intensive process that requires a large amount of training data. After repeating this with hundreds or thousands of training vectors, the weights of the model will eventually converge to fit the task.

\subsubsection{Training with Backpropagation}
One of the most commonly used algorithms to perform the training is the \emph{backpropagation algorithm}~\cite[pp.~151ff]{nn_intro96}. It relies on an \emph{error function} (sometimes also called \emph{loss function}), which is meant to calculate the deviation between the model's current output vector and the desired output vector. The backpropagation algorithm then searches for a minimum of the error function by adjusting the model's weights according to an \emph{optimization strategy}. Which algorithms to choose for both the error function and the optimizing strategy is highly dependent on the task to solve and on the quality requirements for the final predictions.
% TODO: list examples for loss functions and optimizers

The concept of backpropagation is to do a standard forward pass through the model with one batch of data to calculate the output vector and the activations from all the neurons. After that, the predicted output vector is compared to the desired output vector using the error function to get the total error rate. Based on the total error and the activations of the neurons, the backpropagation algorithm now determines the effect each single neuron contributes to the total error rate. In the end, the weights are adjusted in such a way, that the total error rate is likely to decrease.

Sometimes it can happen that the total error rate increases after the weights are adjusted. This is because the backpropagation algorithm only indicates the direction (positive or negative) in which each weight should be moved. But it can not predict the order of magnitude for the adjustment. For that, a parameter called \emph{step width} (often referred to as \emph{learning rate}) is introduced \cite[p.~169]{nn_intro96}. If the learning rate is too big, it can happen that the adjusted weights overshot the minimum of the error function, resulting in an increased error rate. On the other hand, if the learning rate is too small, the training requires even more iterations. Therefore, it is very important to pick an appropriate learning rate for the model.

\WIP{
\begin{itemize}
    % TODO: explain hyperparameters
    \item hyperparameter: LR, epochs, optimizer, etc
    % TODO: explain grid search to examine hyperparameters
    \item grid search, random search
    %TODO: are there other training algorithms?
    \item mention other training algorithms (if there are any)
\end{itemize}
}

\subsubsection{Common Paradigms for Deep Learning}
\label{sec:dl_paradigms}
In general, there are three major paradigms to categorize DNNs based on their learning behaviour~\cite[p.~214f]{dlma14}. First, \emph{supervised learning} improves the network by using predefined pairs of input and output during the training period. This means during training, the expected output (also called \emph{label} or \emph{ground truth}) is directly exposed to the network. Supervised learning fits great for classification tasks, because you can guide the discriminative power of the network right away with the provided labels. In most cases, it needs lots of labelled data to learn from, which usually requires manual work. The architectures presented in sections~\ref{sec:unet} and~\ref{sec:densenet} make use of this learning method.

\emph{Unsupervised learning}, however, does not need any labels for the training process. It is commonly used for tasks like representation learning, trying to compress the input data in a more compact format with only minimal loss in information. Generally speaking, the purpose of unsupervised learning is to find correlations in the dataset, that help forming a deeper understanding of the data. It can also be used to perform unsupervised information clustering or segmentation, as discussed later in section~\ref{sec:w-net}.

The last category is called \emph{reinforcement learning} and is suited for tasks that require interaction with the environment. Based on the actions the network takes, it is either rewarded or punished. For the purpose of this thesis reinforcement learning will not be further explored.

\subsection{Computer Vision Tasks}
There are many different tasks in computer vision for detecting and labelling objects in images. This section will give a brief summary of common tasks in the field. In addition, popular datasets with their scoring methods and suitable approaches to solving the problems are presented.

One of the easier tasks is called \emph{image classification}~\cite[p.~98]{DLbook16}. It is about choosing a matching category from a set of predetermined categories for a single image (see figure~\ref{fig:cv_task_imgclass}). To achieve that, recent approaches use convolutional neural networks as described in section~\ref{sec:cnn}. The output is a probability distribution which indicates the likelihood that the image will fit into a category. The category with the highest probability score can then be selected as the final prediction for the image. A popular dataset for this task is the \emph{MNIST handwritten digits database}~\cite{mnist10}. To measure the performance of the solutions, the challenge recommends to compare the relative error rate of wrong category predictions in the test set.

The next level is to not only assign a label to an image, but also detect where the object in the image is located (see figure~\ref{fig:cv_task_objloc}). Thus, this task is referred to as \emph{object localization}~\cite{rcnn14}. In addition to the category label, the output is extended by a rectangular bounding box that encloses the spatial extent of the object. This change allows to detect multiple objects in the same image, each with a category label and a (possibly overlapping) bounding box. There are different ways to solve this task, like for example bounding box regression~\cite{obj_detection13} or region proposal networks~\cite{ff-rcnn14}. The \emph{ImageNet} dataset presented in~\cite{imgnet09} provides millions of images with prepared object categories and bounding boxes. The performance measurement for this dataset is calculated by matching the predicted objects for an image and evaluating the overlapping area in the bounding boxes.

Rectangular boxes are not very accurate for expressing the spatial extents of an object. Therefore, another task called \emph{semantic segmentation}~\cite{weakseg15} deals with pixel-level classification of an image (see figure~\ref{fig:cv_task_semseg}). This determines a precise mask telling for each pixel of the image the object it belongs to. Since this is is one of the main tasks of this thesis, an in-depth discussion of modern solutions to this task follows in section~\ref{sec:ref_archs}. One dataset to measure the performance of the various approaches is the Pascal Visual Object Classes as laid out in~\cite{pascal_voc15}. The scoring is done by calculating the \emph{intersection over union} (IoU), which gives the percentage of overlap for the predicted mask compared to the actual mask (see equation~\ref{eq:iou}).

% TODO: maybe move this to a new section: "2.3.3 Metrics for CNNs/segmentation"
\begin{equation}
    \label{eq:iou}
    \text{IoU} = \frac{\text{target} \cap \text{prediction}}{\text{target} \cup \text{prediction}}
\end{equation}

The last task to mention here is called \emph{instance segmentation}~\cite{mask-rcnn14}. Whereas previously only one category had to be assigned to each pixel, a distinction is now also made between pixels that belong to the same object category but different instances of an object (see figure~\ref{fig:cv_task_inseg}). This task is quite challenging, as for example parts of an instance can be hidden by another instance of the same category, which leads to gaps in the mask. \emph{Mask R-CNN}~\cite{mask-rcnn14} is a widely used meta-algorithm for solving this task. It combines region proposals used for object localization with semantic segmentation processes. A great dataset for this challenge is the \emph{Microsoft Common Objects in Context} set, which can be found in~\cite{coco15}.

\begin{figure}
    \newcommand{\VisionTasksImageWidth}{0.4\textwidth}
    \centering
    \hfill
    \begin{subfigure}{\VisionTasksImageWidth}
        \includegraphics[width=\textwidth]{images/vision_task_1}
        \caption{Image Classification}
        \label{fig:cv_task_imgclass}
    \end{subfigure}
    \hfill
    \begin{subfigure}{\VisionTasksImageWidth}
        \includegraphics[width=\textwidth]{images/vision_task_2}
        \caption{Object Localization}
        \label{fig:cv_task_objloc}
    \end{subfigure}
    \hfill

    \hfill
    \begin{subfigure}{\VisionTasksImageWidth}
        \includegraphics[width=\textwidth]{images/vision_task_3}
        \caption{Semantic Segmentation}
        \label{fig:cv_task_semseg}
    \end{subfigure}
    \hfill
    \begin{subfigure}{\VisionTasksImageWidth}
        \includegraphics[width=\textwidth]{images/vision_task_4}
        \caption{Instance Segmentation}
        \label{fig:cv_task_inseg}
    \end{subfigure}
    \hfill

    \caption{Computer Vision Tasks~\cite{coco15}}
    \label{fig:cv_tasks}
\end{figure}

\subsection{Convolutional Neural Networks}
\label{sec:cnn}

One subclass of neural networks is called \emph{Convolutional Neural Network} (CNN) ~\cite[p.~359]{praxiseinstieg_ml17}. These types of networks show great results on data with a grid-like topology. Thus, they are often used for processing of image or video data. Traditionally, the layers in DNNs are densely connected, meaning every neuron of one layer is connected to every neuron in the next layer. For grid-like data this is not very efficient, because cells close to each other are often more likely to be correlated. Instead of dense connections, CNNs use operations called \emph{convolution} and \emph{pooling}, which allow to take spatial properties of the input data into account. The following sections explain the commonly used operations in CNNs.
% TODO: name general architecture with conv, pool and fc layers
% TODO: go more into detail, why is CNN for grid-like good, why is dense for grid-like bad?

\subsubsection{Convolution}
\label{sec:convolution}
% TODO: explain depthwise/pointwise convolution difference \cite{depthwise_conv17}
% TODO: explain transposed convolution
% TODO: be more precise with input/output of neurons
\emph{Convolution} is a mathematical operation that uses weighted point-by-point multiplication of two matrices resulting in a scalar value. The concept is shown in figure~\ref{fig:convolution}. The notation $n_{k,~i,~j}$ represents a neuron at row~$i$ and column~$j$ in layer~$k$. For a convolutional layer, neuron $n_{k,~i,~j}$ is connected to all neurons $n_{k-1,~i,~j}$ to $n_{k-1,~i + f_w -1,~j + f_h -1}$. The neurons in layer~$k-1$ are therefore called receptive field of $n_{k,~i,~j}$ with a kernel width of $f_w$ and kernel height of $f_h$.~\cite[p.~361 f]{praxiseinstieg_ml17}

% TODO replace image with proper illustration
\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{images/convolution_template}
    \caption{Receptive field of a convolution operation}
    \label{fig:convolution}
\end{figure}

There are different ways to handle the edges of the grid. If no special action is taken, the output layer will be smaller in terms of edge length. For this reason \emph{zero padding} is often used to enlarge the input layer before convolution is performed. With an appropriate padding it is possible to preserve the width and height of the layer. Another parameter that affect the size of the output layer is called \emph{stride}, that controls the distance between to neighbouring receptive fields.~\cite[p.~361]{praxiseinstieg_ml17}

The trainable parameters in a convolution layer are the weights and biases used for the matrix multiplication introduced earlier. One specific set of weights and biases is called a \emph{filter}. It is responsible for detecting one single feature of the input layer. The output of a filter is thus referred to as \emph{feature map}. To inspect multiple features of the input, a convolution layer usually consists of multiple filters that are trained and calculated independently. In the end, the output of a convolution layer are multiple feature maps, each highlighting one single feature of the input.~\cite[p.~363 f]{praxiseinstieg_ml17}

As stated by Goodfellow et al.\ in~\cite{DLbook16} the convolution operation has some properties that are highly valuable to build an efficient and powerful neural network. Compared to traditional DNNs, convolution layers have only few connections between to subsequent layers. This is because the kernel is much smaller than the input. Especially for images, which can have millions of pixels, this helps to reduce the number of parameters to train. Furthermore, the convolution reuses the same parameters for an entire feature map. Besides increasing the statistical efficiency of the model, it also makes the operation equivariant to translation in the input.

% TODO: explain the above section in more detail?
\begin{comment}
    \cite{DLbook16}
    330: NN use only matrix multiplication. meaning every output unit of one layer interacts with the all input units of the next layer. convolution leverages sparse interactions. for example images can have millions of pixels, but to detect edges it is enough to only look at a few pixels at a time. thus we can have a kernel smaller than the image size resulting in fewer parameters. This reduces memory requirements of the model and increases statistical efficiency.

    331, 333: another advantage is parameter sharing. with matrix multiplication you usually have a weight matrix, where each value is used only once. convolution operation uses parameter sharing, because one kernel is applied multiple times on the same image. and since kernel is smaller than image, it again reduces number of parameters by a significant amount.

    334f: due to parameter sharing, convolution operation is equivariant to translation. Meaning, if you move parts of the input convolution will still give you the same output, just with the moved detection. This is especially helpful for working with images, as object might be in different locations of the image, but should still be realized.
\end{comment}

\subsubsection{Pooling}
\label{sec:pooling}
Unlike convolution, which is used to extract features from the input, \emph{pooling} removes information from the data to reduce the number of parameters and also to prevent overfitting. The procedure of pooling is similar to convolution. However, it does not use matrix multiplication but instead performs a predefined mathematical function on the input matrix. While any function can be used for the pooling operation, most of the times it is the $\max$ function. Thus, it reduces the available information but still keeps the most important activations.~\cite[p.~369 f]{praxiseinstieg_ml17}

Because the function to use is predefined, pooling layers do not contain any trainable parameters. But still they are configurable with the same hyperparameters as convolution layers, namely kernel size, stride and padding. In addition, you can not only do pooling along the two axis of the input, but also along the third axis, which are the feature maps.~\cite[p.~370]{praxiseinstieg_ml17}

% TODO replace image with proper illustration
\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{images/maxpool_template}
    \caption{Max pooling}
    \label{fig:pooling}
\end{figure}

\subsubsection{Performance Metrics}
A major part in the design of neural networks is the evaluation of a model's performance. There are numerous metrics and indicators to assess how well a model operates on a given set of data. They heavily depend on the task which the model is trying to solve. For this thesis we focus on two metrics, both of which are used to gauge results for semantic segmentation challenges.

The \emph{categorical accuracy} measures the ratio between correct label predictions compared to the total number of predictions made~\cite{tf_whitepaper15}. Predictions are considered to be correct if the class with the highest score in the prediction matches the class of the label. In the context of segmentation of a single image, categorical accuracy can be seen as the percentage of pixels that were predicted correctly (see equation~\ref{eq:categorical_accuracy}).

\begin{equation}
    \label{eq:categorical_accuracy}
    \text{Accuracy} = \frac{\text{num. correct predictions}}{\text{num. total predictions}}
\end{equation}

Although this metric is generally useful for a first impression of performance, it has a major drawback when it comes to class imbalances~\cite{tds_segmentation18}. If there is a dominant class, which takes up a majority of the pixels (like e.~g. a background class), the performance estimation of categorical accuracy can be very misleading. Figure~\ref{fig:categorical_accuracy_drawbacks} demonstrates such a case. The categorical accuracy is computed to be around $95\%$ and seems very promising. But a look at the actual predictions show that the model clearly missed the developer's intent. The consequence is, that a high categorical accuracy does not necessarily imply strong segmentation results.

\begin{figure}
    \newcommand{\CategoricalAccuracyImageWidth}{0.3\textwidth}

    \centering
    \hfill
    \begin{subfigure}{\CategoricalAccuracyImageWidth}
        \includegraphics[width=\textwidth]{images/categorical_accuracy_image}
        \caption{Original Image}
        \label{fig:ca_image}
    \end{subfigure}
    \hfill
    \begin{subfigure}{\CategoricalAccuracyImageWidth}
        \includegraphics[width=\textwidth]{images/categorical_accuracy_label}
        \caption{Ground Truth}
        \label{fig:ca_truth}
    \end{subfigure}
    \hfill
    \begin{subfigure}{\CategoricalAccuracyImageWidth}
        \includegraphics[width=\textwidth]{images/categorical_accuracy_prediction}
        \caption{Model Prediction}
        \label{fig:ca_prediction}
    \end{subfigure}
    \hfill
    \caption{Drawbacks for categorical accuracy regarding dominant classes~\cite{tds_segmentation18}}
    \label{fig:categorical_accuracy_drawbacks}
\end{figure}

There are some ways to overcome this issue, for example to calculate and check the categorical accuracy for each class separately. However, in general it is recommended to also integrate other metrics into the evaluation process in order to obtain a wider perspective.

The \emph{intersection over union} (IoU) is another metric which is commonly used to evaluate for segmentation results~\cite{pascal_voc15}. It is computed using two values:
\begin{enumerate}
    \item the area of intersection of the prediction with the ground truth, which is equal to the number of correctly predicted pixels
    \item and the area of union of both prediction and ground truth, which includes correct predictions as well as false positives and false negatives.
\end{enumerate}

The IoU is defined as the ratio between those two values (see equation~\ref{eq:mean_iou}). For multi-class challenges, the IoU is computed separately for each class and then averaged over all classes. This is then called \emph{Mean IoU}.

\begin{equation}
    \label{eq:mean_iou}
    \text{IoU} = \frac{\text{true pos.}}{\text{true pos.}+\text{false pos.}+\text{false neg.}}
\end{equation}

Coming back to the example in figure~\ref{fig:categorical_accuracy_drawbacks}, the Mean IoU is at around $47\%$ (assuming $\sim 0\%$ for ships and $\sim 95\%$ for background). While the metric also does not form a perfect indicator for real-world usability of the predictions, it still directly correlates to segmentation quality. This means a high Mean IoU will always have strong segmentation results.

\subsection{Reference Architectures}
\label{sec:ref_archs}
CNNs play a key role in all computer vision tasks. While a DNN is able to compute a non-linear function, a CNN with a proper architecture allows to compute a non-linear filter. That is the case if the network only consists of layers that operate on the spatial features only (i.~e. no fully-connected layers).

The idea of fully convolutional networks (FCNs) was first introduced by Long et al.\ in~\cite{fcn15}. They took existing CNNs architectures like AlexNet~\cite{alexnet12} and transformed them to the FCN concept by replacing all fully-connected layers with 1x1 convolution layers. That way, the spatial nature of the input was preserved and the output basically formed a heatmap highlighting the parts of the input that the prediction was based on. Later on, they used convolution layers with fractional strides as a way to upsample the resolution with trainable filters. Eventually, this led from the coarse heatmaps to class predictions at the pixel level, which is essentially a semantic segmentation.

Since then, many network architectures have been explored. The architecture affects the quality of the network significantly, thus it is important to choose a matching architecture for the question at hand. There is plenty of research going on for architectural patterns that benefit towards semantic segmentation. In this thesis, three well-performing network architectures have been picked to explore their capabilities of carrying out land use segmentation. The following subsections will present these architectures in detail.

\subsubsection{U-Net Architecture}
\label{sec:unet}
%TODO: go more into detail on the training and the results they achieved
In 2015, Ronneberger et al.\ introduced an architecture called \emph{U-Net} which they recommended for binary segmentation of biomedical images~\cite{unet15}. It consists of a contracting path in the beginning, in which convolution and pooling layers are used to condense the input. The second half forms an expansive part, that uses convolution and upsampling layers to go back to the scale of the original image. Note that the outputs of the convolution layers in the contracting path are cropped and then concatenated to the input layers with the corresponding size in the expansive path. These connections between layers that are not directly adjacent to each other are called \emph{skip connections}. By doing that, the expansive layers still retain enough information from the original input to deliver a precise segmentation.

As can be seen in figure~\ref{fig:unet_architecture}, the name U-Net derives from the U-shape of the network model. The input layer expects a 572x572 pixels image with only one feature channel (i.~e. the inputs are grayscale images). The contracting path consists of downsampling blocks with repeated 3x3 unpadded convolutions with ReLU activation followed by a 2x2 max pooling layer. After each downsampling block, the number of feature maps is doubled. In total, there are four downsampling blocks resulting in 32x32 pixels resolution with 1024 feature maps at the lowest level.

For the expansive path, an upsampling layer followed by a 2x2 convolution layer is used to increase the spatial extents again. At the same time, each upsampling step halves the number of feature maps and then concatenates the (cropped) feature maps of the corresponding size of the contracting path. After that, two 3x3 unpadded convolutions with ReLU activations are applied to form one upsampling block. In total, there are four upsampling blocks eventually landing at a resolution of 388x388 pixels. Finally, a single 1x1 convolution layer with softmax activation carries out the actual segmentation. Therefore, the number of feature maps for this convolution matches the number of classes to predict.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{images/u-net-architecture}
    \caption{U-Net Architecture \cite{unet15}}
    \label{fig:unet_architecture}
\end{figure}

The U-Net architecture contains neither fully connected layers nor do they use padding for the edges in the convolutional layers. Thus, the edges of the image are being cropped and the resulting segmentation map has a lower resolution than the original image. With that strategy, the edges of the predicted segmentation tend to be more accurate, because they still have all the contextual information (i.~e. their surrounding pixels) available. By using a tiling strategy with overlapping tiles, this approach can produce seamless segmentation maps for input images of arbitrary size. For the edges of the input image, Ronneberger et al.\ suggest a mirroring strategy to extrapolate the missing context.~\cite{unet15}

\subsubsection{DenseNet Architecture}
\label{sec:densenet}
As the depth of CNN architectures tends to increase, a new challenge arose regarding the training of those networks. Since the hidden layer can be very far away from the input and output layers, the values and gradients passed between the layers can get lost on the way. In an attempt to solve this, Huang et al.\ came up with the idea to directly connect all layers with matching feature map sizes. They published their architecture under the name \emph{DenseNet} in~\cite{densenet18}.

Huang et al.\ consider the values passed between layers to be the network's state. To access the state of one of the early layers in the network with a subsequent layer, all the layers in between have to pass the state unchanged, thus creating redundancy. With the DenseNet architecture, the state of all previous layers is explicitly passed to subsequent layers by concatenating the feature maps. This means, the network now differentiates between information that originates from an earlier layer and new information produced in the current layer. It allows to keep the convolution layers narrow (i.~e. the number of feature maps), because each layer only adds information, but never changes information that was previously acquired. This concept leads to having fewer trainable parameters and an improved flow of gradients, which both reduce the effort for training the network.~\cite{densenet18}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{images/dense-net-architecture}
    \caption{A 5-layer dense block~\cite{densenet18}}
    \label{fig:dense_block}
\end{figure}

To still be able to use pooling layers to condense the information in the network, the authors introduce a component called \emph{dense block} (see figure~\ref{fig:dense_block}). Inside of a dense block, they apply dense connections between all subsequent convolution layers. Each square in the figure represents one feature map. The initial input of the dense block consists of six feature maps. Those are passed into a group of batch normalization, ReLU activation and convolution layers (marked with $H_1$ in figure~\ref{fig:dense_block}). The convolution layer produces four new feature maps. The next group of layers ($H_2$) now receives all ten feature maps, again producing four new feature maps. For each group of layers, the number of feature maps increases by four. Huang et al.\ therefore called this factor the \emph{growth rate} of the network.

The overall architecture of DenseNets is made up of multiple dense blocks. In between two consecutive dense blocks, there is a \emph{transition layer}, which is formed by batch normalization, 1x1 convolution and 2x2 average pooling. Since the original DenseNet architecture was designed for the purpose of image classification, the conclusion of the network is a 7x7 global average pooling layer and eventually a fully-connected layer with softmax activation. Thus, the original architecture is incapable of performing semantic segmentation.

However, it is possible to extent the architecture with an upsampling path at the end. By doing that, DenseNets can also be used to enact on segmentation tasks. This approach was first investigated by Jégou et al.~\cite{denseseg17}. Their overall architecture looks very similar to the U-Net architecture (see figure~\ref{fig:densenet_segmentation}). But instead of using standalone convolution layers, the authors used the dense blocks proposed in~\cite{densenet18}. This results in a very densely connected architecture, because now there are connections between all convolution layers as well as the skip connections between the downsampling and upsampling path.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{images/dense_segmentation_architecture}
    \caption{DenseNet architecture for semantic segmentation~\cite{denseseg17}}
    \label{fig:densenet_segmentation}
\end{figure}

For this architecture, there are two types of transition layers. The \emph{transition down} layer is used for the downsampling path and has the same setup as the transition layer in the original DenseNet. On the other hand, there is a \emph{transition up} layer, which is used in the upsampling path. It consists of a 3x3 transposed convolution with a stride of 2x2. Thus, the transition up layers double the spatial extents of the feature maps.

In \cite{denseseg17}, Jégou et al.\ proposed a configuration for the DenseNet consisting of five dense blocks each followed by transition down as downsampling path, one intermediate dense block to act as a bottleneck layer, and again five dense blocks each followed by transition up as upsampling path. The final layer is a 1x1 convolution layer with softmax activation and $k$ (the number of classes) feature maps. In total, this network contains 103 convolutional layers.

\subsubsection{W-Net}
\label{sec:w-net}
Both U-Net and DenseNet architectures are based on the idea of supervised learning. There are also some reference architectures that approach the image segmentation task with unsupervised learning. The \emph{W-Net} architecture presented in~\cite{wnet17} is one example.

In general, the W-Net architecture consists of two major parts. The first part generates a $k$-category segmentation of the input image. Subsequently, the second part tries to reconstruct the original image only from the segmentation. The loss function then calculates the deviation between the reconstructed and the original image, i.~e. there is no ground truth required. Because of that, the network detects all segmentation categories on its own by clustering the pixels into similar groups. The downside is, that the chosen categories may be completely useless considering the question at hand.

Figure~\ref{fig:wnet_architecture} illustrates the W-Net architecture. The network is split into an encoder part $U_{Enc}$ and a decoder part $U_{Dec}$. The parts themselves are very much based on the U-Net architecture introduced in section~\ref{sec:unet}. Each part has its own contracting path followed by an expansive path with skip connections between layers with matching spatial extents.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/w-net-architecture}
    \caption{W-Net architecture~\cite{wnet17}}
    \label{fig:wnet_architecture}
\end{figure}

The last layer of the encoder part is a 1x1 convolution layer. The number of feature maps in this layer (denoted with $K$ in figure~\ref{fig:wnet_architecture}) represents the number of categories to use for the segmentation maps. The output of the encoder is a one hot encoded vector with the likelihood for each pixel to belong to a certain category. During training, this vector serves as input for the decoder part of the network. The last layer of the decoder (and thus the last layer of the whole network) is also a 1x1 convolution layer. This layer always consists of three feature maps to restore the color bands of the original image.

As one important difference to the initial U-Net architecture, W-Net uses depthwise separable convolution layers~\cite{depthwise_conv17} in some places instead of regular convolution layers. Regular convolution applies the convolution filters to both the spatial dimensions and the depth dimension (i.~e. the channels of the image) at the same time. In depthwise separable convolution, those two steps are performed separately. The first set of convolution filters is applied to each feature channel separately. Thus, these filters do not change the depth of the image. After that, $n$ filters of size $1\times 1\times c$ are used to condense all $c$ feature channels into a single channel each, resulting in $n$ feature maps in total. By doing that, the network differentiates between spatial correlations and cross-channel correlations independently. This increases the computational efficiency of the model without any significant impact on the effectiveness.

In their paper~\cite{wnet17}, Xia and Kulis propose to follow up the initial segmentation of W-Net with two postprocessing steps. First, they foster sharp edges for object boundaries by using conditional random fields~\cite{crf17}. In the second step, they reduce over-segmentation of the image by merging segments according to weighted boundary maps~\cite{hierarchy_imgseg11}. The postprocessing will not be further employed throughout this thesis.


\subsection{Architecture Comparison}
Overall, all of the discussed reference architectures follow the \emph{encoder-decoder} architectural pattern. In the encoder, the number of feature maps is increased while the spatial dimensions are reduced. In a symmetrical fashion, the decoder decreases the number of feature maps while increasing the spatial dimensions. This is a very common pattern that is used by many image segmentation architectures~\cite{imseg_survey20}. It enables to capture enough context for each pixel and still provides precise localization.

It is difficult to compare the actual performance of the three architectures, since they were all trained and evaluated on different datasets with differing metrics. However, some metrics are reviewed considering the context and complexity of their particular challenge. Additionally, figure~\ref{fig:segmentation_examples} provides visual examples of the segmentation results of the three different approaches.

U-Net was trained and evaluated for the ISBI cell tracking challenge~\cite{isbi_challenge}. The challenge consists of two datasets, both showing biological cells scanned with different microscopy techniques. For both datasets the main objective was to provide a binary classification for the cell images, where a distinction between cell and non-cell pixels must be made. Also the precise detection of boundaries between adjacent cells was a key point of interest for this challenge. The U-Net model with around $31$ million parameters achieved an average IoU of $92.03\%$ for the PhC-U373 and $77.56\%$ for the DIC-HeLa datasets~\cite{unet15}. One example segmentation for the DIC-HeLa dataset is shown in Figure~\ref{fig:exseg_unet}.

To train and evaluate the FC-DenseNet the CamVid dataset~\cite{camvid_challenge} was used. CamVid is a database of urban scene videos, fully labelled with 32 different classes. The classes include moving objects like pedestrians and cars, ceilings like sky and tunnel as well as fixed objects like buildings, roads and fences. The authors of FC-DenseNet report a mean IoU of $66.9\%$ and a global accuracy of $91.5\%$~\cite{denseseg17}. Only looking at the plain numbers, FC-DenseNet seems worse than U-Net because of the lower mean IoU. However, with only around $9.4$ million parameters it requires significantly less computational power. Also, the CamVid dataset is much more sophisticated compared to the ISBI cell tracking challenge. As can be seen in figure~\ref{fig:exseg_densenet}, the segmentation predictions of FC-DenseNet are of high quality.

W-Net was trained on the PASCAL VOC 2012 dataset~\cite{pascal_voc12} and evaluated with the Berkeley Segmentation Database (BSDS500)~\cite{hierarchy_imgseg11}. Both datasets contain hundreds of images with precisely labelled segmentation maps. But since the W-Net architecture is trained in an unsupervised approach, the labels of the datasets are only used for evaluation. Also, the performance metrics are different, because there are multiple ground-truth labels for each image in the dataset. For example, they use a metric called \emph{segmentation covering}~(SC)~\cite{hierarchy_imgseg11}, which is an extension to the well-known IoU metric.

Without postprocessing, W-Net scores $62\%$ SC on the BSDS500 dataset. Again, according to the plain numbers this seems worse than the results of U-Net and DenseNet. But taking into account, that W-Net was trained with unsupervised learning, i.~e. it did not receive any labelling information during training, the results are quite remarkable. Figure~\ref{fig:exseg_wnet} presents a segmentation map predicted by W-Net.

Overall, it is difficult to judge which architecture has delivered the best results in general. Likewise, it is not possible to draw conclusions about which architecture works best on the given problem of land coverage segmentation. Therefore in section~\ref{sec:segmentation} all three architectures are implemented and evaluated for this specific task.

\begin{figure}
    \newcommand{\SegmentationExampleImageWidth}{0.3\textwidth}
    \centering
    \hfill
    \begin{subfigure}{\SegmentationExampleImageWidth}
        \includegraphics[width=\textwidth]{images/segmentation_example_unet}
        \caption{U-Net~\cite{unet15}}
        \label{fig:exseg_unet}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth]{images/segmentation_example_densenet}
        \caption{DenseNet~\cite{denseseg17}}
        \label{fig:exseg_densenet}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.21\textwidth}
        \includegraphics[width=\textwidth]{images/segmentation_example_wnet}
        \caption{W-Net~\cite{wnet17}}
        \label{fig:exseg_wnet}
    \end{subfigure}
    \hfill
    \caption{Segmentation results from the presented architectures}
    \label{fig:segmentation_examples}
\end{figure}


\newpage